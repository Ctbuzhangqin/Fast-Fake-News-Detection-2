{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、环境设置\n",
    "本示例基于飞桨开源框架2.0版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "from paddle.nn import Conv2D, Linear, Embedding\r\n",
    "from paddle import to_tensor\r\n",
    "import paddle.nn.functional as F\r\n",
    "import os, zipfile\r\n",
    "import io, random, json\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "print(paddle.__version__)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、数据准备\n",
    "\n",
    "（1）解压数据，读取并解析数据，生成all_data.txt\n",
    "\n",
    "（2）生成数据字典，即dict.txt\n",
    "\n",
    "（3）生成数据列表，并进行训练集与验证集的划分，train_list.txt 、eval_list.txt\n",
    "\n",
    "（4）定义训练数据集提供器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#解压原始数据集，将Rumor_Dataset.zip解压至data目录下\n",
    "src_path=\"data/data20519/Rumor_Dataset.zip\"\n",
    "target_path=\"/home/aistudio/data/Chinese_Rumor_Dataset-master\"\n",
    "if(not os.path.isdir(target_path)):\n",
    "    z = zipfile.ZipFile(src_path, 'r')\n",
    "    z.extractall(path=target_path)\n",
    "    z.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "#数据文件路径\r\n",
    "rumor_class_dirs = os.listdir(target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/rumor-repost/\")\r\n",
    "\r\n",
    "non_rumor_class_dirs = os.listdir(target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/non-rumor-repost/\")\r\n",
    "original_microblog = target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/original-microblog/\"\r\n",
    "\r\n",
    "rumor_label=\"0\"\r\n",
    "non_rumor_label=\"1\"\r\n",
    "\r\n",
    "\r\n",
    "rumor_num = 0\r\n",
    "non_rumor_num = 0\r\n",
    "all_rumor_list = []\r\n",
    "all_non_rumor_list = []\r\n",
    "\r\n",
    "#解析数据\r\n",
    "for rumor_class_dir in rumor_class_dirs: \r\n",
    "    if(rumor_class_dir != '.DS_Store'):\r\n",
    "        #遍历数据，并解析\r\n",
    "        with open(original_microblog + rumor_class_dir, 'r') as f:\r\n",
    "\t        rumor_content = f.read()\r\n",
    "        rumor_dict = json.loads(rumor_content)\r\n",
    "        all_rumor_list.append(rumor_label+\"\\t\"+rumor_dict[\"text\"]+\"\\n\")\r\n",
    "        rumor_num +=1\r\n",
    "\r\n",
    "#解析数据\r\n",
    "for non_rumor_class_dir in non_rumor_class_dirs: \r\n",
    "    if(non_rumor_class_dir != '.DS_Store'):\r\n",
    "        with open(original_microblog + non_rumor_class_dir, 'r') as f2:\r\n",
    "\t        non_rumor_content = f2.read()\r\n",
    "        non_rumor_dict = json.loads(non_rumor_content)\r\n",
    "        all_non_rumor_list.append(non_rumor_label+\"\\t\"+non_rumor_dict[\"text\"]+\"\\n\")\r\n",
    "        non_rumor_num +=1\r\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#全部数据进行乱序后写入all_data.txt\r\n",
    "\r\n",
    "data_list_path=\"/home/aistudio/data/\"\r\n",
    "all_data_path=data_list_path + \"all_data.txt\"\r\n",
    "\r\n",
    "all_data_list = all_rumor_list + all_non_rumor_list\r\n",
    "\r\n",
    "random.shuffle(all_data_list)\r\n",
    "\r\n",
    "#在生成all_data.txt之前，首先将其清空\r\n",
    "with open(all_data_path, 'w') as f:\r\n",
    "    f.seek(0)\r\n",
    "    f.truncate() \r\n",
    "    \r\n",
    "with open(all_data_path, 'a') as f:\r\n",
    "    for data in all_data_list:\r\n",
    "        f.write(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 生成数据字典\r\n",
    "def create_dict(data_path, dict_path):\r\n",
    "    with open(dict_path, 'w') as f:\r\n",
    "        f.seek(0)\r\n",
    "        f.truncate() \r\n",
    "\r\n",
    "    dict_set = set()\r\n",
    "    # 读取全部数据\r\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\r\n",
    "        lines = f.readlines()\r\n",
    "    # 把数据生成一个元组\r\n",
    "    for line in lines:\r\n",
    "        content = line.split('\\t')[-1].replace('\\n', '')\r\n",
    "        for s in content:\r\n",
    "            dict_set.add(s)\r\n",
    "    # 把元组转换成字典，一个字对应一个数字\r\n",
    "    dict_list = []\r\n",
    "    i = 0\r\n",
    "    for s in dict_set:\r\n",
    "        dict_list.append([s, i])\r\n",
    "        i += 1\r\n",
    "    # 添加未知字符\r\n",
    "    dict_txt = dict(dict_list)\r\n",
    "    end_dict = {\"<unk>\": i}\r\n",
    "    dict_txt.update(end_dict)\r\n",
    "    end_dict = {\"<pad>\": i+1}\r\n",
    "    dict_txt.update(end_dict)\r\n",
    "    # 把这些字典保存到本地中\r\n",
    "    with open(dict_path, 'w', encoding='utf-8') as f:\r\n",
    "        f.write(str(dict_txt))\r\n",
    "        \r\n",
    "    print(\"数据字典生成完成！\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建序列化表示的数据,并按照一定比例划分训练数据train_list.txt与验证数据eval_list.txt\r\n",
    "def load_vocab(file_path):\r\n",
    "    fr = open(file_path, 'r', encoding='utf8')\r\n",
    "    vocab = eval(fr.read())   #读取的str转换为字典\r\n",
    "    fr.close()\r\n",
    "\r\n",
    "    return vocab\r\n",
    "\r\n",
    "def f_write_txt(words, dict_txt, label):\r\n",
    "    labs = \"\"\r\n",
    "    for s in words:\r\n",
    "        lab = str(dict_txt[s])\r\n",
    "        labs = labs + lab + ','\r\n",
    "    labs = labs[:-1]\r\n",
    "    labs = labs + '\\t' + label + '\\n'\r\n",
    "    return labs\r\n",
    "\r\n",
    "def create_data_list(data_path, train_path, test_path, dict_path):\r\n",
    "    \r\n",
    "    dict_txt = load_vocab(dict_path)\r\n",
    "    with open(data_path, 'r', encoding='utf-8') as f_data:\r\n",
    "        lines = f_data.readlines()\r\n",
    "\r\n",
    "    i = 0\r\n",
    "    maxlen = 0\r\n",
    "    with open(test_path, 'a', encoding='utf-8') as f_eval,open(train_path, 'a', encoding='utf-8') as f_train:\r\n",
    "        for line in lines:\r\n",
    "            words = line.split('\\t')[-1].replace('\\n', '')\r\n",
    "            maxlen = max(maxlen, len(words))\r\n",
    "            label = line.split('\\t')[0]\r\n",
    "            labs = f_write_txt(words, dict_txt, label)\r\n",
    "            # 每8个 抽取一个数据用于验证\r\n",
    "            if i % 7 == 0:\r\n",
    "                f_eval.write(labs)\r\n",
    "            else:\r\n",
    "                f_train.write(labs)\r\n",
    "            i += 1\r\n",
    "    print(\"数据列表生成完成！\")\r\n",
    "    print(maxlen)\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据字典生成完成！\n",
      "数据列表生成完成！\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "# 把生成的数据列表都放在自己的总类别文件夹中\r\n",
    "data_root_path = \"/home/aistudio/data/\" \r\n",
    "data_path = os.path.join(data_root_path, 'all_data.txt')\r\n",
    "train_path = os.path.join(data_list_path, 'train_list.txt')\r\n",
    "test_path = os.path.join(data_list_path, 'eval_list.txt')\r\n",
    "dict_path = os.path.join(data_root_path, \"dict.txt\")\r\n",
    "\r\n",
    "# 创建数据字典\r\n",
    "create_dict(data_path, dict_path)\r\n",
    "\r\n",
    "# 创建数据列表\r\n",
    "\r\n",
    "#在生成数据之前，首先将eval_list.txt和train_list.txt清空\r\n",
    "with open(test_path, 'w', encoding='utf-8') as f_eval:\r\n",
    "    f_eval.seek(0)\r\n",
    "    f_eval.truncate()\r\n",
    "with open(train_path, 'w', encoding='utf-8') as f_train:\r\n",
    "    f_train.seek(0)\r\n",
    "    f_train.truncate() \r\n",
    "\r\n",
    "create_data_list(data_path, train_path, test_path, dict_path)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:\n",
      "sentence list id is: ['1900', '176', '3761', '3249', '4400', '4299', '1634', '3455', '3249', '3247', '1882', '1359', '117', '1108', '2191', '1946', '1760', '1170', '1170', '950', '4235', '2598', '1204', '687', '1082', '557', '1082', '3487', '949', '32', '1672', '2844', '3249', '442', '1859', '3973', '2699', '123', '1442', '4299', '1634', '1615', '1193', '951', '110', '1795', '1247', '2026', '514', '1615', '3847', '3557', '4235', '1870', '4120', '1518', '624', '666', '3814', '1297', '2026', '103', '1442', '3119', '497', '1615', '6', '1647', '4299', '1634', '1041', '2315', '1615', '4299', '1634', '1167', '1647', '3455', '1557', '1615', '465', '4011', '1112', '3352', '3585', '1985', '1134', '3677', '2032', '2635', '1204', '687', '1082', '3480', '3249', '4051', '154', '1598', '666', '1744', '624', '666', '2010', '3455', '51', '1442', '4299', '1634', '167', '4242', '2032', '1082', '3487', '3933', '36', '4012', '1268', '3110', '2598', '3460', '2776', '1615', '2327', '557', '1769', '952', '4223', '557', '1951', '4259', '196', '2032', '2032', '2032', '1882', '487', '1108']\n",
      "sentence list is:  【 碉 堡 了 ： 黄 瓜 断 了 ！ [ 偷 笑 ] 】 宁 波 X X 职 高 某 女 同 学 在 学 校 食 堂 购 买 了 一 根 大 号 型 的 黄 瓜 ，   回 到 寝 室 自 慰 ， 由 于 高 潮 来 临 无 法 控 制 自 己 的 情 绪 ， 突 然 黄 瓜 咔 嚓 ， 黄 瓜 竟 然 断 掉 ， 很 难 想 象 这 个 力 量 。 那 女 同 学 用 了 各 种 方 法 都 无 法 将 断 裂 的 黄 瓜 取 出 。 学 校 又 送 入 上 海 某 医 院 ， 现 在 手 术 还 在 进 行 中 。 。 。 [ 衰 ]\n",
      "sentence label id is: 0\n",
      "---------------------------------\n",
      "2:\n",
      "sentence list id is: ['1882', '856', '2576', '1108', '1010', '1112', '557', '550', '2099', '550', '2239', '532', '2893', '2845', '3110', '3688', '162', '3192', '3269', '1615', '2216', '442', '1985', '4401', '3594', '922', '655', '2861', '503', '3009', '3302', '402', '1442', '409', '2125', '1615', '1926', '2388', '2576', '1923', '4122', '2848', '1747', '3838', '2417', '1207', '856', '3531', '1615', '1843', '4229', '1061', '856', '2576', '673', '3002', '3697', '3973', '269', '4357', '3909', '2845', '1374', '3697', '3342', '2038', '3609', '3432', '1442', '1615', '4093', '390', '1903', '3390', '3837', '2032']\n",
      "sentence list is:  [ 耕 读 ] 设 想 在 北 京 北 边 郊 区 如 海 淀 或 昌 平 ， 找 一 个 风 景 美 丽 、 空 气 清 新 的 村 庄 ， 若 干 读 书 人 租 住 民 房 和 耕 地 ， 过 传 统 耕 读 生 活 ？ 大 家 以 为 如 何 ？ 有 此 意 向 的 ， 可 跟 贴 附 议 。\n",
      "sentence label id is: 1\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 打印前2条训练数据\r\n",
    "vocab = load_vocab(dict_path)\r\n",
    "\r\n",
    "def ids_to_str(ids):\r\n",
    "    words = []\r\n",
    "    for k in ids:\r\n",
    "        w = list(vocab.keys())[list(vocab.values()).index(int(k))]\r\n",
    "        words.append(w if isinstance(w, str) else w.decode('ASCII'))\r\n",
    "    return \" \".join(words)\r\n",
    "\r\n",
    "with io.open(train_path, \"r\", encoding='utf8') as fin:\r\n",
    "        i = 0\r\n",
    "        for line in fin:\r\n",
    "            i += 1\r\n",
    "            cols = line.strip().split(\"\\t\")\r\n",
    "            if len(cols) != 2:\r\n",
    "                sys.stderr.write(\"[NOTICE] Error Format Line!\")\r\n",
    "                continue\r\n",
    "            label = int(cols[1])\r\n",
    "            wids = cols[0].split(\",\")\r\n",
    "            print(str(i)+\":\")\r\n",
    "            print('sentence list id is:', wids)\r\n",
    "            print('sentence list is: ', ids_to_str(wids))\r\n",
    "            print('sentence label id is:', label)\r\n",
    "            print('---------------------------------')\r\n",
    "            \r\n",
    "            if i == 2: break\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============train_dataset =============\n",
      "[1900  176 3761 3249 4400 4299 1634 3455 3249 3247 1882 1359  117 1108\n",
      " 2191 1946 1760 1170 1170  950 4235 2598 1204  687 1082  557 1082 3487\n",
      "  949   32 1672 2844 3249  442 1859 3973 2699  123 1442 4299 1634 1615\n",
      " 1193  951  110 1795 1247 2026  514 1615 3847 3557 4235 1870 4120 1518\n",
      "  624  666 3814 1297 2026  103 1442 3119  497 1615    6 1647 4299 1634\n",
      " 1041 2315 1615 4299 1634 1167 1647 3455 1557 1615  465 4011 1112 3352\n",
      " 3585 1985 1134 3677 2032 2635 1204  687 1082 3480 3249 4051  154 1598\n",
      "  666 1744  624  666 2010 3455   51 1442 4299 1634  167 4242 2032 1082\n",
      " 3487 3933   36 4012 1268 3110 2598 3460 2776 1615 2327  557 1769  952\n",
      " 4223  557 1951 4259  196 2032 2032 2032 1882  487 1108 4410 4410 4410\n",
      " 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410]\n",
      "(150,)\n",
      "[0]\n",
      "=============test_dataset =============\n",
      "[1882 2087 2868 1108 1882 2087 2868 1108 1900  307 2903 1306 4066 2648\n",
      " 2244 3828 3650  151 1082  673 1193 1544 4122 3972 4156 2779 3061  955\n",
      " 4257 3056  221 2191 4270 1748  880 1268 1615 2915  250  307 2903 2779\n",
      " 1306 4066 2639 1134  560  666 1615 2648 2244 3828 3650  151  601 1082\n",
      "  810 1442  444 2241 1082  673 2032  632 1748 1268 2736 1615 2244 2569\n",
      "   18  319 2861  687 1082 4235 3508 3382 1748  990 4011 4312 2809 1442\n",
      " 1197 3227 1615  557 2779 3061 3315 1501 1683 3314 1615 2679 4044 3621\n",
      " 3056  560  666 2861 3892 2535 1060 1769 3247 3430 1544 2779 3838 1043\n",
      " 4120 1250 2359 1615 3973 3270  335 3384 2861 2690  335 1350 3272 2552\n",
      " 2955 1234 1615 2010  955 4257 3056  221 1442 2779 3838 2947 4259 3338\n",
      " 2501 2106 4410 4410 4410 4410 4410 4410 4410 4410]\n",
      "(150,)\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "\r\n",
    "\r\n",
    "class RumorDataset(paddle.io.Dataset):\r\n",
    "    def __init__(self, data_dir):\r\n",
    "        self.data_dir = data_dir\r\n",
    "        self.all_data = []\r\n",
    "       \r\n",
    "        with io.open(self.data_dir, \"r\", encoding='utf8') as fin:\r\n",
    "            for line in fin:\r\n",
    "                cols = line.strip().split(\"\\t\")\r\n",
    "                if len(cols) != 2:\r\n",
    "                    sys.stderr.write(\"[NOTICE] Error Format Line!\")\r\n",
    "                    continue\r\n",
    "                label = []\r\n",
    "                label.append(int(cols[1]))\r\n",
    "                wids = cols[0].split(\",\")\r\n",
    "                if len(wids)>=150:\r\n",
    "                    wids = np.array(wids[:150]).astype('int64')     \r\n",
    "                else:\r\n",
    "                    wids = np.concatenate([wids, [vocab[\"<pad>\"]]*(150-len(wids))]).astype('int64')\r\n",
    "                label = np.array(label).astype('int64')\r\n",
    "                self.all_data.append((wids, label))\r\n",
    "\r\n",
    "        \r\n",
    "    def __getitem__(self, index):\r\n",
    "        data, label = self.all_data[index]\r\n",
    "        return data, label\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.all_data)\r\n",
    "\r\n",
    "\r\n",
    "batch_size = 32\r\n",
    "train_dataset = RumorDataset(train_path)\r\n",
    "test_dataset = RumorDataset(test_path)\r\n",
    "\r\n",
    "train_loader = paddle.io.DataLoader(train_dataset, places=paddle.CPUPlace(), return_list=True,\r\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
    "test_loader = paddle.io.DataLoader(test_dataset, places=paddle.CPUPlace(), return_list=True,\r\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#check\r\n",
    "\r\n",
    "print('=============train_dataset =============') \r\n",
    "for data, label in train_dataset:\r\n",
    "    print(data)\r\n",
    "    print(np.array(data).shape)\r\n",
    "    print(label)\r\n",
    "    break\r\n",
    "\r\n",
    "\r\n",
    "print('=============test_dataset =============') \r\n",
    "for data, label in test_dataset:\r\n",
    "    print(data)\r\n",
    "    print(np.array(data).shape)\r\n",
    "    print(label)\r\n",
    "    break\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#定义卷积网络\r\n",
    "class Conv_FFD(paddle.nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(Conv_FFD,self).__init__()\r\n",
    "        self.dict_dim = vocab[\"<pad>\"]\r\n",
    "        self.emb_dim = 128\r\n",
    "        self.hid_dim = 128\r\n",
    "        self.fc_hid_dim = 96\r\n",
    "        self.class_dim = 2\r\n",
    "        self.channels = 1\r\n",
    "        self.win_size = [3, self.hid_dim]\r\n",
    "        self.batch_size = 32\r\n",
    "        self.seq_len = 150\r\n",
    "        self.embedding = Embedding(self.dict_dim + 1, self.emb_dim, sparse=False)\r\n",
    "        self.hidden1 = paddle.nn.Conv2D(in_channels=1,                        #通道数\r\n",
    "                                            out_channels=self.hid_dim,        #卷积核个数\r\n",
    "                                            kernel_size=self.win_size,        #卷积核大小\r\n",
    "                                            padding=[1, 1]\r\n",
    "                                            )                         \r\n",
    "        self.relu1 = paddle.nn.ReLU()\r\n",
    "        self.hidden3 = paddle.nn.MaxPool2D(kernel_size=2,         #池化核大小\r\n",
    "                                            stride=2)             #池化步长2\r\n",
    "        self.hidden4 = paddle.nn.Linear(128*75, 2)\r\n",
    "    #网络的前向计算过程\r\n",
    "    def forward(self,input):\r\n",
    "        \r\n",
    "        #print('输入维度：', input.shape)\r\n",
    "        x = self.embedding(input)\r\n",
    "        x = paddle.reshape(x, [32, 1, 150, 128])   \r\n",
    "        x = self.hidden1(x)\r\n",
    "        x = self.relu1(x)\r\n",
    "        #print('第一层卷积输出维度：', x.shape)\r\n",
    "        x = self.hidden3(x)\r\n",
    "        #print('池化后输出维度：', x.shape)\r\n",
    "        #在输入全连接层时，需将特征图拉平会自动将数据拉平.\r\n",
    "\r\n",
    "        x = paddle.reshape(x, shape=[self.batch_size, -1])\r\n",
    "        out = self.hidden4(x)\r\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, batch_id: 0, loss is: [0.6900476]\n",
      "epoch: 0, batch_id: 50, loss is: [0.7203587]\n",
      "[validation] accuracy: 0.824999988079071, loss: 0.47299620509147644\n",
      "epoch: 1, batch_id: 0, loss is: [0.3159663]\n",
      "epoch: 1, batch_id: 50, loss is: [0.26881313]\n",
      "[validation] accuracy: 0.8416666388511658, loss: 0.3673580586910248\n",
      "epoch: 2, batch_id: 0, loss is: [0.11671057]\n",
      "epoch: 2, batch_id: 50, loss is: [0.05700203]\n"
     ]
    }
   ],
   "source": [
    "def draw_process(title,color,iters,data,label):\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"iter\", fontsize=20)\n",
    "    plt.ylabel(label, fontsize=20)\n",
    "    plt.plot(iters, data,color=color,label=label) \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\n",
    "    steps = 0\n",
    "    Iters, total_loss, total_acc = [], [], []\n",
    "    for epoch in range(3):\n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            steps += 1\n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "            logits = model(sent)\n",
    "            loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "            if batch_id % 50 == 0:\n",
    "                Iters.append(steps)\n",
    "                total_loss.append(loss.numpy()[0])\n",
    "                total_acc.append(acc.numpy()[0])\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, loss.numpy()))\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        # evaluate model after one epoch\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for batch_id, data in enumerate(test_loader):\n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "            logits = model(sent)\n",
    "            loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "        \n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "        print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "        model.train()\n",
    "\n",
    "    paddle.save(model.state_dict(),\"model_final.pdparams\")\n",
    "    draw_process(\"trainning loss\",\"red\",Iters,total_loss,\"trainning loss\")\n",
    "    draw_process(\"trainning acc\",\"green\",Iters,total_acc,\"trainning acc\")\n",
    "        \n",
    "#model = CNN()\n",
    "model = Conv_FFD()\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 五、模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\r\n",
    "模型评估\r\n",
    "'''\r\n",
    "model_state_dict = paddle.load('model_final.pdparams')\r\n",
    "model = Conv_FFD()\r\n",
    "model.set_state_dict(model_state_dict) \r\n",
    "model.eval()\r\n",
    "accuracies = []\r\n",
    "losses = []\r\n",
    "\r\n",
    "for batch_id, data in enumerate(test_loader):\r\n",
    "    \r\n",
    "    sent = data[0]\r\n",
    "    label = data[1]\r\n",
    "\r\n",
    "    logits = model(sent)\r\n",
    "    loss = paddle.nn.functional.cross_entropy(logits, label)\r\n",
    "    acc = paddle.metric.accuracy(logits, label)\r\n",
    "    \r\n",
    "    accuracies.append(acc.numpy())\r\n",
    "    losses.append(loss.numpy())\r\n",
    "\r\n",
    "avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\r\n",
    "print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "from sklearn.metrics import confusion_matrix\r\n",
    "model_state_dict = paddle.load('model_final.pdparams')\r\n",
    "model = Conv_FFD()\r\n",
    "model.set_state_dict(model_state_dict) \r\n",
    "model.eval()\r\n",
    "predictions = []\r\n",
    "r = []\r\n",
    "for batch_id, data in enumerate(test_loader):\r\n",
    "    \r\n",
    "    sent = data[0]\r\n",
    "    gt_labels = data[1].numpy()\r\n",
    "    for i in gt_labels:\r\n",
    "        r.append(i)\r\n",
    "    results = model(sent)\r\n",
    "    for probs in results:\r\n",
    "        # 映射分类label\r\n",
    "        idx = np.argmax(probs)\r\n",
    "        predictions.append(idx)\r\n",
    "    \r\n",
    "confusion_matrix(r, predictions)\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "target_names = [\"0\",\"1\"]\r\n",
    "CR=classification_report(r, predictions, target_names=target_names)\r\n",
    "print(CR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 六、模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "label_map = {0:\"谣言\", 1:\"不是谣言\"}\r\n",
    "\r\n",
    "model_state_dict = paddle.load('model_final.pdparams')\r\n",
    "model = Conv_FFD()\r\n",
    "model.set_state_dict(model_state_dict) \r\n",
    "model.eval()\r\n",
    "\r\n",
    "for batch_id, data in enumerate(test_loader):\r\n",
    "    \r\n",
    "    sent = data[0]\r\n",
    "    gt_labels = data[1].numpy()\r\n",
    "   \r\n",
    "    results = model(sent)\r\n",
    "\r\n",
    "    predictions = []\r\n",
    "    for probs in results:\r\n",
    "        # 映射分类label\r\n",
    "        idx = np.argmax(probs)\r\n",
    "        labels = label_map[idx]\r\n",
    "        predictions.append(labels)\r\n",
    "    \r\n",
    "    for i,pre in enumerate(predictions):\r\n",
    "        print('数据: {} \\n\\n预测: {} \\n原始标签：{}'.format(ids_to_str(sent[0]).replace(\" \", \"\").replace(\"<pad>\",\"\"), pre, label_map[gt_labels[0][0]]))\r\n",
    "        break\r\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
