{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 任务描述：\n",
    "社交媒体的发展在加速信息传播的同时，也带来了虚假谣言信息的泛滥，往往会引发诸多不安定因素，并对经济和社会产生巨大的影响。\n",
    "\n",
    "2016年美国总统大选期间，受访选民平均每人每天接触到4篇虚假新闻，虚假新闻被认为影响了2016年美国大选和英国脱欧的投票结果；近期，在新型冠状病毒感染的肺炎疫情防控的关键期，在全国人民都为疫情揪心时，网上各种有关疫情防控的谣言接连不断，从“广州公交线路因新型冠状病毒肺炎疫情停运”到“北京市为防控疫情采取封城措施”，从“钟南山院士被感染”到“10万人感染肺炎”等等，这些不切实际的谣言，“操纵”了舆论感情，误导了公众的判断，更影响了社会稳定。\n",
    "\n",
    "人们常说“流言止于智者”，要想不被网上的流言和谣言盅惑、伤害，首先需要对其进行科学甄别，而时下人工智能正在尝试担任这一角色。那么，在打假一线AI技术如何做到去伪存真？\n",
    "\n",
    "传统的谣言检测模型一般根据谣言的内容、用户属性、传播方式人工地构造特征，而人工构建特征存在考虑片面、浪费人力等现象。本次实践使用基于循环神经网络（RNN）的谣言检测模型，将文本中的谣言事件向量化，通过循环神经网络的学习训练来挖掘表示文本深层的特征，避免了特征构建的问题，并能发现那些不容易被人发现的特征，从而产生更好的效果。\n",
    "\n",
    "数据集介绍：\n",
    "\n",
    "本次实践所使用的数据是从新浪微博不实信息举报平台抓取的中文谣言数据，数据集中共包含1538条谣言和1849条非谣言。如下图所示，每条数据均为json格式，其中text字段代表微博原文的文字内容。\n",
    "\n",
    "更多数据集介绍请参考https://github.com/thunlp/Chinese_Rumor_Dataset。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/30665456670941acaf0ad4bfa78252e8d44f296dda8d48dea2ada26a5f10ef1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、环境设置\n",
    "本示例基于飞桨开源框架2.0版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "import paddle\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "print(paddle.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、数据准备\n",
    "\n",
    "（1）解压数据，读取并解析数据，生成all_data.txt\n",
    "\n",
    "（2）生成数据字典，即dict.txt\n",
    "\n",
    "（3）生成数据列表，并进行训练集与验证集的划分，train_list.txt 、eval_list.txt\n",
    "\n",
    "（4）定义训练数据集提供器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "src_path=\"data/data20519/Rumor_Dataset.zip\"\n",
    "target_path=\"/home/aistudio/data/Chinese_Rumor_Dataset-master\"\n",
    "if(not os.path.isdir(target_path)):\n",
    "    z = zipfile.ZipFile(src_path, 'r')\n",
    "    z.extractall(path=target_path)\n",
    "    z.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "谣言数据总量为：1538\n",
      "非谣言数据总量为：1849\n"
     ]
    }
   ],
   "source": [
    "import io\r\n",
    "import random\r\n",
    "import json\r\n",
    "\r\n",
    "\r\n",
    "#谣言数据文件路径\r\n",
    "rumor_class_dirs = os.listdir(target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/rumor-repost/\")\r\n",
    "\r\n",
    "#非谣言数据文件路径\r\n",
    "non_rumor_class_dirs = os.listdir(target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/non-rumor-repost/\")\r\n",
    "\r\n",
    "original_microblog = target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/original-microblog/\"\r\n",
    "\r\n",
    "#谣言标签为0，非谣言标签为1\r\n",
    "rumor_label=\"0\"\r\n",
    "non_rumor_label=\"1\"\r\n",
    "\r\n",
    "#分别统计谣言数据与非谣言数据的总数\r\n",
    "rumor_num = 0\r\n",
    "non_rumor_num = 0\r\n",
    "\r\n",
    "all_rumor_list = []\r\n",
    "all_non_rumor_list = []\r\n",
    "\r\n",
    "#解析谣言数据\r\n",
    "for rumor_class_dir in rumor_class_dirs: \r\n",
    "    if(rumor_class_dir != '.DS_Store'):\r\n",
    "        #遍历谣言数据，并解析\r\n",
    "        with open(original_microblog + rumor_class_dir, 'r') as f:\r\n",
    "\t        rumor_content = f.read()\r\n",
    "        rumor_dict = json.loads(rumor_content)\r\n",
    "        all_rumor_list.append(rumor_label+\"\\t\"+rumor_dict[\"text\"]+\"\\n\")\r\n",
    "        rumor_num +=1\r\n",
    "\r\n",
    "#解析非谣言数据\r\n",
    "for non_rumor_class_dir in non_rumor_class_dirs: \r\n",
    "    if(non_rumor_class_dir != '.DS_Store'):\r\n",
    "        with open(original_microblog + non_rumor_class_dir, 'r') as f2:\r\n",
    "\t        non_rumor_content = f2.read()\r\n",
    "        non_rumor_dict = json.loads(non_rumor_content)\r\n",
    "        all_non_rumor_list.append(non_rumor_label+\"\\t\"+non_rumor_dict[\"text\"]+\"\\n\")\r\n",
    "        non_rumor_num +=1\r\n",
    "        \r\n",
    "print(\"谣言数据总量为：\"+str(rumor_num))\r\n",
    "print(\"非谣言数据总量为：\"+str(non_rumor_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#全部数据进行乱序后写入all_data.txt\r\n",
    "\r\n",
    "data_list_path=\"/home/aistudio/data/\"\r\n",
    "all_data_path=data_list_path + \"all_data.txt\"\r\n",
    "\r\n",
    "all_data_list = all_rumor_list + all_non_rumor_list\r\n",
    "\r\n",
    "random.shuffle(all_data_list)\r\n",
    "\r\n",
    "#在生成all_data.txt之前，首先将其清空\r\n",
    "with open(all_data_path, 'w') as f:\r\n",
    "    f.seek(0)\r\n",
    "    f.truncate() \r\n",
    "    \r\n",
    "with open(all_data_path, 'a') as f:\r\n",
    "    for data in all_data_list:\r\n",
    "        f.write(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 生成数据字典\r\n",
    "def create_dict(data_path, dict_path):\r\n",
    "    with open(dict_path, 'w') as f:\r\n",
    "        f.seek(0)\r\n",
    "        f.truncate() \r\n",
    "\r\n",
    "    dict_set = set()\r\n",
    "    # 读取全部数据\r\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\r\n",
    "        lines = f.readlines()\r\n",
    "    # 把数据生成一个元组\r\n",
    "    for line in lines:\r\n",
    "        content = line.split('\\t')[-1].replace('\\n', '')\r\n",
    "        for s in content:\r\n",
    "            dict_set.add(s)\r\n",
    "    # 把元组转换成字典，一个字对应一个数字\r\n",
    "    dict_list = []\r\n",
    "    i = 0\r\n",
    "    for s in dict_set:\r\n",
    "        dict_list.append([s, i])\r\n",
    "        i += 1\r\n",
    "    # 添加未知字符\r\n",
    "    dict_txt = dict(dict_list)\r\n",
    "    end_dict = {\"<unk>\": i}\r\n",
    "    dict_txt.update(end_dict)\r\n",
    "    end_dict = {\"<pad>\": i+1}\r\n",
    "    dict_txt.update(end_dict)\r\n",
    "    # 把这些字典保存到本地中\r\n",
    "    with open(dict_path, 'w', encoding='utf-8') as f:\r\n",
    "        f.write(str(dict_txt))\r\n",
    "\r\n",
    "        \r\n",
    "    print(\"数据字典生成完成！\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 创建序列化表示的数据,并按照一定比例划分训练数据train_list.txt与验证数据eval_list.txt\r\n",
    "def create_data_list(data_list_path):\r\n",
    "    #在生成数据之前，首先将eval_list.txt和train_list.txt清空\r\n",
    "    with open(os.path.join(data_list_path, 'eval_list.txt'), 'w', encoding='utf-8') as f_eval:\r\n",
    "        f_eval.seek(0)\r\n",
    "        f_eval.truncate()\r\n",
    "        \r\n",
    "    with open(os.path.join(data_list_path, 'train_list.txt'), 'w', encoding='utf-8') as f_train:\r\n",
    "        f_train.seek(0)\r\n",
    "        f_train.truncate() \r\n",
    "    \r\n",
    "    with open(os.path.join(data_list_path, 'dict.txt'), 'r', encoding='utf-8') as f_data:\r\n",
    "        dict_txt = eval(f_data.readlines()[0])\r\n",
    "\r\n",
    "    with open(os.path.join(data_list_path, 'all_data.txt'), 'r', encoding='utf-8') as f_data:\r\n",
    "        lines = f_data.readlines()\r\n",
    "    \r\n",
    "    i = 0\r\n",
    "    maxlen = 0\r\n",
    "    with open(os.path.join(data_list_path, 'eval_list.txt'), 'a', encoding='utf-8') as f_eval,open(os.path.join(data_list_path, 'train_list.txt'), 'a', encoding='utf-8') as f_train:\r\n",
    "        for line in lines:\r\n",
    "            words = line.split('\\t')[-1].replace('\\n', '')\r\n",
    "            maxlen = max(maxlen, len(words))\r\n",
    "            label = line.split('\\t')[0]\r\n",
    "            labs = \"\"\r\n",
    "            # 每8个 抽取一个数据用于验证\r\n",
    "            if i % 5 == 0:\r\n",
    "                for s in words:\r\n",
    "                    lab = str(dict_txt[s])\r\n",
    "                    labs = labs + lab + ','\r\n",
    "                labs = labs[:-1]\r\n",
    "                labs = labs + '\\t' + label + '\\n'\r\n",
    "                f_eval.write(labs)\r\n",
    "            else:\r\n",
    "                for s in words:\r\n",
    "                    lab = str(dict_txt[s])\r\n",
    "                    labs = labs + lab + ','\r\n",
    "                labs = labs[:-1]\r\n",
    "                labs = labs + '\\t' + label + '\\n'\r\n",
    "                f_train.write(labs)\r\n",
    "            i += 1\r\n",
    "        \r\n",
    "    print(\"数据列表生成完成！\")\r\n",
    "    print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据字典生成完成！\n",
      "数据列表生成完成！\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "# 把生成的数据列表都放在自己的总类别文件夹中\r\n",
    "data_root_path = \"/home/aistudio/data/\" \r\n",
    "data_path = os.path.join(data_root_path, 'all_data.txt')\r\n",
    "dict_path = os.path.join(data_root_path, \"dict.txt\")\r\n",
    "\r\n",
    "# 创建数据字典\r\n",
    "create_dict(data_path, dict_path)\r\n",
    "\r\n",
    "# 创建数据列表\r\n",
    "create_data_list(data_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_vocab(file_path):\r\n",
    "    fr = open(file_path, 'r', encoding='utf8')\r\n",
    "    vocab = eval(fr.read())   #读取的str转换为字典\r\n",
    "    fr.close()\r\n",
    "\r\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:\n",
      "sentence list id is: ['490', '290', '3231', '2320', '2929', '1007', '1455', '1500', '1747', '2343', '1056', '1789', '2320', '2929', '1791', '3062', '3503', '2390', '814', '3368', '2320', '2929', '1007', '1455', '3622', '2497', '4172', '1974', '2433', '2942', '3407', '2320', '2929', '1872', '1613', '309', '4112', '2005', '1714', '3487', '2438', '3503', '2390', '2433', '2942', '4108', '2320', '2929', '1007', '1455', '527', '1634', '3598', '1528', '3198', '1231', '3581', '1007', '3653', '3416', '2837', '114', '2281', '595', '1775', '3406', '2771', '4108', '527', '3598', '3598', '1528', '2320', '2929', '3184', '1622', '4112', '510', '2403', '839', '839', '4108', '335', '1488', '986', '1881', '4172', '1528', '2022', '2655', '3360', '3231', '3038', '1405', '2824', '4108', '4112', '510', '3407', '2320', '2929', '1007', '1455', '4267', '4365', '3598', '2343', '1056', '1887', '2418', '1091', '1617', '1570', '3407', '2024', '1747', '1331', '862', '1920', '1620', '1330', '1528', '1920', '2337', '1081', '1656', '1525', '4252', '827', '1597', '335', '1488', '4112', '510', '2557', '236', '1058', '2350', '2107', '3407']\n",
      "sentence list is:  【 关 于 搜 狐 视 频 最 新 消 息 】 搜 狐 今 日 正 式 公 布 搜 狐 视 频 整 体 人 事 任 命 。 搜 狐 娱 乐 副 总 裁 邓 晔 被 正 式 任 命 为 搜 狐 视 频 C E O ， 凤 凰 卫 视 前 执 行 台 长 @ 刘 春   为 C O O ， 搜 狐 销 售 总 监 崔 莉 莉 为 市 场 负 责 人 ， 原 主 编 于 涛 晋 升 为 总 监 。 搜 狐 视 频 I P O 消 息 将 很 快 揭 晓 。 写 新 闻 的 记 者 们 ， 记 得 加 上 桔 子 酒 店 市 场 总 监 微 博 透 露 啊 。\n",
      "sentence label id is: 1\n",
      "---------------------------------\n",
      "2:\n",
      "sentence list id is: ['490', '2148', '2986', '3114', '4108', '4156', '1144', '828', '4296', '3860', '1685', '1789', '3075', '2085', '4186', '2226', '3225', '186', '3170', '3730', '4201', '1978', '1978', '1528', '2337', '3056', '667', '3938', '952', '1685', '1661', '3491', '3642', '695', '2939', '1528', '3914', '994', '2217', '3695', '3860', '3289', '373', '1776', '2720', '2664', '621', '862', '1968', '642', '1528', '2330', '3457', '3860', '3603', '1247', '862', '4060', '652', '1776', '319', '847', '3123', '3274', '2043', '1503', '1919', '3471', '3778', '2332', '2220', '1919', '3860', '2655', '373', '1776', '3434', '4060', '2872', '1656', '809', '809', '1083', '972', '1697', '170', '3170', '2824', '895', '862', '3778', '211', '1393', '862', '3407', '1276', '3440', '2700', '3440', '3724', '1735', '1528', '4186', '4172', '3561', '3876', '1528', '250', '3114', '2783', '2920', '3407', '2087', '3047', '4274', '498', '325', '424', '1297', '1777', '2860', '862', '2148', '2986', '3114', '1528', '4108', '37', '2347', '4238', '1144', '828', '4296', '3860', '1685']\n",
      "sentence list is:  【 同 学 会 为 啥 变 味 儿 了 ？ 】 ① 第 一 个 念 头 ： 如 果 A A ， 得 花 多 少 钱 ？ ② 先 聊 回 忆 ， 后 面 就 没 了 话 题 ； ③ 客 套 的 寒 暄 ， 像 极 了 应 酬 的 饭 局 ； ④ 介 绍 职 务 、 成 绩 和 资 源 成 了 主 题 ； ⑤ 饭 桌 上 慢 慢 只 剩 两 类 ： 升 官 的 和 发 财 的 。 ⑥ 越 来 越 挑 剔 ， 一 人 否 定 ， 聚 会 告 吹 。 （ 中 国 青 年 报 ） 简 单 的 同 学 会 ， 为 何 感 觉 变 味 儿 了 ？\n",
      "sentence label id is: 1\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 打印前2条训练数据\r\n",
    "vocab = load_vocab(os.path.join(data_root_path, 'dict.txt'))\r\n",
    "\r\n",
    "def ids_to_str(ids):\r\n",
    "    words = []\r\n",
    "    for k in ids:\r\n",
    "        w = list(vocab.keys())[list(vocab.values()).index(int(k))]\r\n",
    "        words.append(w if isinstance(w, str) else w.decode('ASCII'))\r\n",
    "    return \" \".join(words)\r\n",
    "\r\n",
    "file_path = os.path.join(data_root_path, 'train_list.txt')\r\n",
    "with io.open(file_path, \"r\", encoding='utf8') as fin:\r\n",
    "        i = 0\r\n",
    "        for line in fin:\r\n",
    "            i += 1\r\n",
    "            cols = line.strip().split(\"\\t\")\r\n",
    "            if len(cols) != 2:\r\n",
    "                sys.stderr.write(\"[NOTICE] Error Format Line!\")\r\n",
    "                continue\r\n",
    "            label = int(cols[1])\r\n",
    "            wids = cols[0].split(\",\")\r\n",
    "            print(str(i)+\":\")\r\n",
    "            print('sentence list id is:', wids)\r\n",
    "            print('sentence list is: ', ids_to_str(wids))\r\n",
    "            print('sentence label id is:', label)\r\n",
    "            print('---------------------------------')\r\n",
    "            \r\n",
    "            if i == 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============train_dataset =============\n",
      "[ 490  290 3231 2320 2929 1007 1455 1500 1747 2343 1056 1789 2320 2929\n",
      " 1791 3062 3503 2390  814 3368 2320 2929 1007 1455 3622 2497 4172 1974\n",
      " 2433 2942 3407 2320 2929 1872 1613  309 4112 2005 1714 3487 2438 3503\n",
      " 2390 2433 2942 4108 2320 2929 1007 1455  527 1634 3598 1528 3198 1231\n",
      " 3581 1007 3653 3416 2837  114 2281  595 1775 3406 2771 4108  527 3598\n",
      " 3598 1528 2320 2929 3184 1622 4112  510 2403  839  839 4108  335 1488\n",
      "  986 1881 4172 1528 2022 2655 3360 3231 3038 1405 2824 4108 4112  510\n",
      " 3407 2320 2929 1007 1455 4267 4365 3598 2343 1056 1887 2418 1091 1617\n",
      " 1570 3407 2024 1747 1331  862 1920 1620 1330 1528 1920 2337 1081 1656\n",
      " 1525 4252  827 1597  335 1488 4112  510 2557  236 1058 2350 2107 3407\n",
      " 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410]\n",
      "(150,)\n",
      "[1]\n",
      "=============test_dataset =============\n",
      "[ 490  199 3316 2775 3395 1102  233 1989 2265 1897  334  825 2137 1310\n",
      " 1789 3182 4274 2695 4074 1684 1209 1222 2660 3438 1690 3062 1528 2027\n",
      "  943 3438 1690 1310 2205 4272  876 3975 2837  814  454 1310 4275 4246\n",
      " 1212 2837 1528  186 2529 4123 4252 1503 3047 4274 3926 3056  199 3316\n",
      " 3622 1488  910 1310 1123 2438 2904 2519 1745 3278 1528 1500 2763  104\n",
      " 2179 3959 3634 2592 1512 1273 2955 1222 3959 2179 2775 3395 1102  233\n",
      " 1989 2265 1897 1062  879  334  825 2137 1310 1528 3274  552 4398 2517\n",
      " 1188 1308 3768 1212  365 1771  752   11 1310 2137 1310  862 1989 2265\n",
      " 1897 1887 3982 1353 3959 3000   39  651  923 2743 3814  947 2599  902\n",
      " 3999   15   15 1287  834 2955 2955   15 2693 1050 2495 2955 4395 3978\n",
      " 1978 1222 3593 2439 2641 4410 4410 4410 4410 4410]\n",
      "(150,)\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "vocab = load_vocab(os.path.join(data_root_path, 'dict.txt'))\r\n",
    "\r\n",
    "class RumorDataset(paddle.io.Dataset):\r\n",
    "    def __init__(self, data_dir):\r\n",
    "        self.data_dir = data_dir\r\n",
    "        self.all_data = []\r\n",
    "       \r\n",
    "        with io.open(self.data_dir, \"r\", encoding='utf8') as fin:\r\n",
    "            for line in fin:\r\n",
    "                cols = line.strip().split(\"\\t\")\r\n",
    "                if len(cols) != 2:\r\n",
    "                    sys.stderr.write(\"[NOTICE] Error Format Line!\")\r\n",
    "                    continue\r\n",
    "                label = []\r\n",
    "                label.append(int(cols[1]))\r\n",
    "                wids = cols[0].split(\",\")\r\n",
    "                if len(wids)>=150:\r\n",
    "                    wids = np.array(wids[:150]).astype('int64')     \r\n",
    "                else:\r\n",
    "                    wids = np.concatenate([wids, [vocab[\"<pad>\"]]*(150-len(wids))]).astype('int64')\r\n",
    "                label = np.array(label).astype('int64')\r\n",
    "                self.all_data.append((wids, label))\r\n",
    "\r\n",
    "        \r\n",
    "    def __getitem__(self, index):\r\n",
    "        data, label = self.all_data[index]\r\n",
    "        return data, label\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.all_data)\r\n",
    "\r\n",
    "\r\n",
    "batch_size = 32\r\n",
    "train_dataset = RumorDataset(os.path.join(data_root_path, 'train_list.txt'))\r\n",
    "test_dataset = RumorDataset(os.path.join(data_root_path, 'eval_list.txt'))\r\n",
    "\r\n",
    "train_loader = paddle.io.DataLoader(train_dataset, places=paddle.CPUPlace(), return_list=True,\r\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
    "test_loader = paddle.io.DataLoader(test_dataset, places=paddle.CPUPlace(), return_list=True,\r\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#check\r\n",
    "\r\n",
    "print('=============train_dataset =============') \r\n",
    "for data, label in train_dataset:\r\n",
    "    print(data)\r\n",
    "    print(np.array(data).shape)\r\n",
    "    print(label)\r\n",
    "    break\r\n",
    "\r\n",
    "\r\n",
    "print('=============test_dataset =============') \r\n",
    "for data, label in test_dataset:\r\n",
    "    print(data)\r\n",
    "    print(np.array(data).shape)\r\n",
    "    print(label)\r\n",
    "    break\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\r\n",
    "from paddle.nn import Conv2D, Linear, Embedding\r\n",
    "from paddle import to_tensor\r\n",
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "class RNN(paddle.nn.Layer):\r\n",
    "    def __init__(self):\r\n",
    "        super(RNN, self).__init__()\r\n",
    "        self.dict_dim = vocab[\"<pad>\"]\r\n",
    "        self.emb_dim = 128\r\n",
    "        self.hid_dim = 128\r\n",
    "        self.class_dim = 2\r\n",
    "        self.embedding = Embedding(\r\n",
    "            self.dict_dim + 1, self.emb_dim,\r\n",
    "            sparse=False)\r\n",
    "        self._fc1 = Linear(self.emb_dim, self.hid_dim)\r\n",
    "        self.lstm = paddle.nn.LSTM(self.hid_dim, self.hid_dim)\r\n",
    "        self.fc2 = Linear(19200, self.class_dim)\r\n",
    "\r\n",
    "    def forward(self, inputs):\r\n",
    "        # [32, 150]\r\n",
    "        emb = self.embedding(inputs)\r\n",
    "        # [32, 150, 128]\r\n",
    "        fc_1 = self._fc1(emb)\r\n",
    "        # [32, 150, 128]\r\n",
    "        x = self.lstm(fc_1)\r\n",
    "        x = paddle.reshape(x[0], [0, -1])\r\n",
    "        x = self.fc2(x)\r\n",
    "        x = paddle.nn.functional.softmax(x)\r\n",
    "        return x\r\n",
    "\r\n",
    "rnn = RNN()\r\n",
    "paddle.summary(rnn,(32,150),\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_process(title,color,iters,data,label):\r\n",
    "    plt.title(title, fontsize=24)\r\n",
    "    plt.xlabel(\"iter\", fontsize=20)\r\n",
    "    plt.ylabel(label, fontsize=20)\r\n",
    "    plt.plot(iters, data,color=color,label=label) \r\n",
    "    plt.legend()\r\n",
    "    plt.grid()\r\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    model.train()\n",
    "    opt = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\n",
    "    \n",
    "    steps = 0\n",
    "    Iters, total_loss, total_acc = [], [], []\n",
    "    \n",
    "    for epoch in range(3):\n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            steps += 1\n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "            \n",
    "            logits = model(sent)\n",
    "            loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "\n",
    "            if batch_id % 50 == 0:\n",
    "                Iters.append(steps)\n",
    "                total_loss.append(loss.numpy()[0])\n",
    "                total_acc.append(acc.numpy()[0])\n",
    "\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, loss.numpy()))\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        # evaluate model after one epoch\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        \n",
    "        for batch_id, data in enumerate(test_loader):\n",
    "            \n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "\n",
    "            logits = model(sent)\n",
    "            loss = paddle.nn.functional.cross_entropy(logits, label)\n",
    "            acc = paddle.metric.accuracy(logits, label)\n",
    "            \n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "        \n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "\n",
    "        print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "    paddle.save(model.state_dict(),\"model_final.pdparams\")\n",
    "    \n",
    "    draw_process(\"trainning loss\",\"red\",Iters,total_loss,\"trainning loss\")\n",
    "    draw_process(\"trainning acc\",\"green\",Iters,total_acc,\"trainning acc\")\n",
    "        \n",
    "import time\n",
    "start_time=time.time()\n",
    "model = RNN()\n",
    "train(model)\n",
    "end_time=time.time()\n",
    "running_time=end_time-start_time\n",
    "print(running_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 五、模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "from sklearn.metrics import confusion_matrix\r\n",
    "model_state_dict = paddle.load('model_final.pdparams')\r\n",
    "model = RNN()\r\n",
    "model.set_state_dict(model_state_dict) \r\n",
    "model.eval()\r\n",
    "predictions = []\r\n",
    "r = []\r\n",
    "for batch_id, data in enumerate(test_loader):\r\n",
    "    \r\n",
    "    sent = data[0]\r\n",
    "    gt_labels = data[1].numpy()\r\n",
    "    for i in gt_labels:\r\n",
    "        r.append(i)\r\n",
    "    results = model(sent)\r\n",
    "    for probs in results:\r\n",
    "        # 映射分类label\r\n",
    "        idx = np.argmax(probs)\r\n",
    "        predictions.append(idx)\r\n",
    "    \r\n",
    "confusion_matrix(r, predictions)\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "target_names = [\"0\",\"1\"]\r\n",
    "CR=classification_report(r, predictions, target_names=target_names,digits=4)\r\n",
    "print(CR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 六、模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\r\n",
    "\r\n",
    "label_map = {0:\"是\", 1:\"否\"}\r\n",
    "\r\n",
    "model_state_dict = paddle.load('model_final.pdparams')\r\n",
    "model = RNN()\r\n",
    "model.set_state_dict(model_state_dict) \r\n",
    "model.eval()\r\n",
    "\r\n",
    "for batch_id, data in enumerate(test_loader):\r\n",
    "    \r\n",
    "    sent = data[0]\r\n",
    "    results = model(sent)\r\n",
    "\r\n",
    "    predictions = []\r\n",
    "    for probs in results:\r\n",
    "        # 映射分类label\r\n",
    "        idx = np.argmax(probs)\r\n",
    "        labels = label_map[idx]\r\n",
    "        predictions.append(labels)\r\n",
    "    \r\n",
    "    for i,pre in enumerate(predictions):\r\n",
    "        print('数据: {} \\n\\n是否谣言: {}'.format(ids_to_str(sent[0]), pre))\r\n",
    "        break\r\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
