{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 任务描述：\n",
    "社交媒体的发展在加速信息传播的同时，也带来了虚假谣言信息的泛滥，往往会引发诸多不安定因素，并对经济和社会产生巨大的影响。\n",
    "\n",
    "2016年美国总统大选期间，受访选民平均每人每天接触到4篇虚假新闻，虚假新闻被认为影响了2016年美国大选和英国脱欧的投票结果；近期，在新型冠状病毒感染的肺炎疫情防控的关键期，在全国人民都为疫情揪心时，网上各种有关疫情防控的谣言接连不断，从“广州公交线路因新型冠状病毒肺炎疫情停运”到“北京市为防控疫情采取封城措施”，从“钟南山院士被感染”到“10万人感染肺炎”等等，这些不切实际的谣言，“操纵”了舆论感情，误导了公众的判断，更影响了社会稳定。\n",
    "\n",
    "人们常说“流言止于智者”，要想不被网上的流言和谣言盅惑、伤害，首先需要对其进行科学甄别，而时下人工智能正在尝试担任这一角色。那么，在打假一线AI技术如何做到去伪存真？\n",
    "\n",
    "传统的谣言检测模型一般根据谣言的内容、用户属性、传播方式人工地构造特征，而人工构建特征存在考虑片面、浪费人力等现象。本次实践使用基于循环神经网络（RNN）的谣言检测模型，将文本中的谣言事件向量化，通过循环神经网络的学习训练来挖掘表示文本深层的特征，避免了特征构建的问题，并能发现那些不容易被人发现的特征，从而产生更好的效果。\n",
    "\n",
    "数据集介绍：\n",
    "\n",
    "本次实践所使用的数据是从新浪微博不实信息举报平台抓取的中文谣言数据，数据集中共包含1538条谣言和1849条非谣言。如下图所示，每条数据均为json格式，其中text字段代表微博原文的文字内容。\n",
    "\n",
    "更多数据集介绍请参考https://github.com/thunlp/Chinese_Rumor_Dataset。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/30665456670941acaf0ad4bfa78252e8d44f296dda8d48dea2ada26a5f10ef1a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、环境设置\n",
    "本示例基于飞桨开源框架2.0版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-20T09:14:30.191926Z",
     "iopub.status.busy": "2022-10-20T09:14:30.191240Z",
     "iopub.status.idle": "2022-10-20T09:14:32.781574Z",
     "shell.execute_reply": "2022-10-20T09:14:32.780565Z",
     "shell.execute_reply.started": "2022-10-20T09:14:30.191877Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/__init__.py:107: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import MutableMapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/rcsetup.py:20: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable, Mapping\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/matplotlib/colors.py:53: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.2\n"
     ]
    }
   ],
   "source": [
    "import paddle as pd\n",
    "import paddle.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from paddlenlp.transformers import BertForSequenceClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、数据准备\n",
    "\n",
    "（1）解压数据，读取并解析数据，生成all_data.txt\n",
    "\n",
    "（2）生成数据字典，即dict.txt\n",
    "\n",
    "（3）生成数据列表，并进行训练集与验证集的划分，train_list.txt 、eval_list.txt\n",
    "\n",
    "（4）定义训练数据集提供器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-15T12:43:47.710256Z",
     "iopub.status.busy": "2022-10-15T12:43:47.709194Z",
     "iopub.status.idle": "2022-10-15T12:43:51.317878Z",
     "shell.execute_reply": "2022-10-15T12:43:51.316740Z",
     "shell.execute_reply.started": "2022-10-15T12:43:47.710218Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, zipfile\n",
    "src_path=\"data/data170001/Rumor_Dataset.zip\"\n",
    "target_path=\"/home/aistudio/data/Chinese_Rumor_Dataset-master\"\n",
    "if(not os.path.isdir(target_path)):\n",
    "    z = zipfile.ZipFile(src_path, 'r')\n",
    "    z.extractall(path=target_path)\n",
    "    z.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-15T12:43:53.025923Z",
     "iopub.status.busy": "2022-10-15T12:43:53.025169Z",
     "iopub.status.idle": "2022-10-15T12:43:53.179016Z",
     "shell.execute_reply": "2022-10-15T12:43:53.178292Z",
     "shell.execute_reply.started": "2022-10-15T12:43:53.025881Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "谣言数据总量为：1538\n",
      "非谣言数据总量为：1849\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "#谣言数据文件路径\n",
    "rumor_class_dirs = os.listdir(target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/rumor-repost/\")\n",
    "\n",
    "#非谣言数据文件路径\n",
    "non_rumor_class_dirs = os.listdir(target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/non-rumor-repost/\")\n",
    "\n",
    "original_microblog = target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/original-microblog/\"\n",
    "\n",
    "#谣言标签为0，非谣言标签为1\n",
    "rumor_label=\"0\"\n",
    "non_rumor_label=\"1\"\n",
    "\n",
    "#分别统计谣言数据与非谣言数据的总数\n",
    "rumor_num = 0\n",
    "non_rumor_num = 0\n",
    "\n",
    "all_rumor_list = []\n",
    "all_non_rumor_list = []\n",
    "\n",
    "#解析谣言数据\n",
    "for rumor_class_dir in rumor_class_dirs: \n",
    "    if(rumor_class_dir != '.DS_Store'):\n",
    "        #遍历谣言数据，并解析\n",
    "        with open(original_microblog + rumor_class_dir, 'r') as f:\n",
    "\t        rumor_content = f.read()\n",
    "        rumor_dict = json.loads(rumor_content)\n",
    "        all_rumor_list.append(rumor_label+\"\\t\"+rumor_dict[\"text\"]+\"\\n\")\n",
    "        rumor_num +=1\n",
    "\n",
    "#解析非谣言数据\n",
    "for non_rumor_class_dir in non_rumor_class_dirs: \n",
    "    if(non_rumor_class_dir != '.DS_Store'):\n",
    "        with open(original_microblog + non_rumor_class_dir, 'r') as f2:\n",
    "\t        non_rumor_content = f2.read()\n",
    "        non_rumor_dict = json.loads(non_rumor_content)\n",
    "        all_non_rumor_list.append(non_rumor_label+\"\\t\"+non_rumor_dict[\"text\"]+\"\\n\")\n",
    "        non_rumor_num +=1\n",
    "        \n",
    "print(\"谣言数据总量为：\"+str(rumor_num))\n",
    "print(\"非谣言数据总量为：\"+str(non_rumor_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-15T12:43:57.130064Z",
     "iopub.status.busy": "2022-10-15T12:43:57.129107Z",
     "iopub.status.idle": "2022-10-15T12:43:57.140575Z",
     "shell.execute_reply": "2022-10-15T12:43:57.139931Z",
     "shell.execute_reply.started": "2022-10-15T12:43:57.130029Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#全部数据进行乱序后写入all_data.txt\n",
    "\n",
    "data_list_path=\"/home/aistudio/data/\"\n",
    "all_data_path=data_list_path + \"all_data.txt\"\n",
    "\n",
    "all_data_list = all_rumor_list + all_non_rumor_list\n",
    "\n",
    "random.shuffle(all_data_list)\n",
    "\n",
    "#在生成all_data.txt之前，首先将其清空\n",
    "with open(all_data_path, 'w') as f:\n",
    "    f.seek(0)\n",
    "    f.truncate() \n",
    "    \n",
    "with open(all_data_path, 'a') as f:\n",
    "    for data in all_data_list:\n",
    "        f.write(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-15T12:44:00.164983Z",
     "iopub.status.busy": "2022-10-15T12:44:00.164298Z",
     "iopub.status.idle": "2022-10-15T12:44:00.172345Z",
     "shell.execute_reply": "2022-10-15T12:44:00.171520Z",
     "shell.execute_reply.started": "2022-10-15T12:44:00.164948Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 生成数据字典\n",
    "def create_dict(data_path, dict_path):\n",
    "    with open(dict_path, 'w') as f:\n",
    "        f.seek(0)\n",
    "        f.truncate() \n",
    "\n",
    "    dict_set = set()\n",
    "    # 读取全部数据\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    # 把数据生成一个元组\n",
    "    for line in lines:\n",
    "        content = line.split('\\t')[-1].replace('\\n', '')\n",
    "        for s in content:\n",
    "            dict_set.add(s)\n",
    "    # 把元组转换成字典，一个字对应一个数字\n",
    "    dict_list = []\n",
    "    i = 0\n",
    "    for s in dict_set:\n",
    "        dict_list.append([s, i])\n",
    "        i += 1\n",
    "    # 添加未知字符\n",
    "    dict_txt = dict(dict_list)\n",
    "    end_dict = {\"<unk>\": i}\n",
    "    dict_txt.update(end_dict)\n",
    "    end_dict = {\"<pad>\": i+1}\n",
    "    dict_txt.update(end_dict)\n",
    "    # 把这些字典保存到本地中\n",
    "    with open(dict_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(str(dict_txt))\n",
    "\n",
    "        \n",
    "    print(\"数据字典生成完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-15T13:41:31.626619Z",
     "iopub.status.busy": "2022-10-15T13:41:31.625864Z",
     "iopub.status.idle": "2022-10-15T13:41:31.635826Z",
     "shell.execute_reply": "2022-10-15T13:41:31.635214Z",
     "shell.execute_reply.started": "2022-10-15T13:41:31.626591Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 创建序列化表示的数据,并按照一定比例划分训练数据train_list.txt与验证数据eval_list.txt\n",
    "def create_data_list(data_list_path):\n",
    "    #在生成数据之前，首先将eval_list.txt和train_list.txt清空\n",
    "    with open(os.path.join(data_list_path, 'eval_list.txt'), 'w', encoding='utf-8') as f_eval:\n",
    "        f_eval.seek(0)\n",
    "        f_eval.truncate()\n",
    "        \n",
    "    with open(os.path.join(data_list_path, 'train_list.txt'), 'w', encoding='utf-8') as f_train:\n",
    "        f_train.seek(0)\n",
    "        f_train.truncate() \n",
    "    \n",
    "    with open(os.path.join(data_list_path, 'dict.txt'), 'r', encoding='utf-8') as f_data:\n",
    "        dict_txt = eval(f_data.readlines()[0])\n",
    "\n",
    "    with open(os.path.join(data_list_path, 'all_data.txt'), 'r', encoding='utf-8') as f_data:\n",
    "        lines = f_data.readlines()\n",
    "    \n",
    "    i = 0\n",
    "    maxlen = 0\n",
    "    with open(os.path.join(data_list_path, 'eval_list.txt'), 'a', encoding='utf-8') as f_eval,open(os.path.join(data_list_path, 'train_list.txt'), 'a', encoding='utf-8') as f_train:\n",
    "        for line in lines:\n",
    "            words = line.split('\\t')[-1].replace('\\n', '')\n",
    "            maxlen = max(maxlen, len(words))\n",
    "            label = line.split('\\t')[0]\n",
    "            labs = \"\"\n",
    "            # 每8个 抽取一个数据用于验证\n",
    "            if i % 8 == 0:    # n=5\n",
    "                for s in words:\n",
    "                    lab = str(dict_txt[s])\n",
    "                    labs = labs + lab + ','\n",
    "                labs = labs[:-1]\n",
    "                labs = labs + '\\t' + label + '\\n'\n",
    "                f_eval.write(labs)\n",
    "            else:\n",
    "                for s in words:\n",
    "                    lab = str(dict_txt[s])\n",
    "                    labs = labs + lab + ','\n",
    "                labs = labs[:-1]\n",
    "                labs = labs + '\\t' + label + '\\n'\n",
    "                f_train.write(labs)\n",
    "            i += 1\n",
    "        \n",
    "    print(\"数据列表生成完成！\")\n",
    "    print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-15T13:41:34.024511Z",
     "iopub.status.busy": "2022-10-15T13:41:34.023752Z",
     "iopub.status.idle": "2022-10-15T13:41:34.250527Z",
     "shell.execute_reply": "2022-10-15T13:41:34.249807Z",
     "shell.execute_reply.started": "2022-10-15T13:41:34.024482Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据字典生成完成！\n",
      "数据列表生成完成！\n",
      "226\n"
     ]
    }
   ],
   "source": [
    "# 把生成的数据列表都放在自己的总类别文件夹中\n",
    "data_root_path = \"/home/aistudio/data/\" \n",
    "data_path = os.path.join(data_root_path, 'all_data.txt')\n",
    "dict_path = os.path.join(data_root_path, \"dict.txt\")\n",
    "\n",
    "# 创建数据字典\n",
    "create_dict(data_path, dict_path)\n",
    "\n",
    "# 创建数据列表\n",
    "create_data_list(data_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-15T13:41:35.664574Z",
     "iopub.status.busy": "2022-10-15T13:41:35.663861Z",
     "iopub.status.idle": "2022-10-15T13:41:35.668073Z",
     "shell.execute_reply": "2022-10-15T13:41:35.667463Z",
     "shell.execute_reply.started": "2022-10-15T13:41:35.664545Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(file_path):\n",
    "    fr = open(file_path, 'r', encoding='utf8')\n",
    "    vocab = eval(fr.read())   #读取的str转换为字典\n",
    "    fr.close()\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-15T13:41:37.389921Z",
     "iopub.status.busy": "2022-10-15T13:41:37.389235Z",
     "iopub.status.idle": "2022-10-15T13:41:37.448051Z",
     "shell.execute_reply": "2022-10-15T13:41:37.447449Z",
     "shell.execute_reply.started": "2022-10-15T13:41:37.389895Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:\n",
      "sentence list id is: ['1269', '1785', '184', '2662', '1370', '2351', '3181', '592', '2208', '3924', '1268', '884', '4153', '2363', '1369', '2403', '2662', '1472', '3698', '1153', '2978', '9', '3648', '3658', '2662', '3560', '1522', '339', '4254', '709', '3560', '1522', '2627', '1179', '3241', '3281', '323', '1431', '2480', '2627', '1179', '3832', '2287', '1199', '1816', '3182', '3243', '2662', '433', '1364', '3340', '3925', '2662', '3916', '433', '609', '3560', '3916', '1933', '406', '3673', '2146', '4114', '1878', '4153', '2185', '646', '2480', '1116', '1370', '2351', '3181', '1785', '3181', '3061', '2208', '1221', '2627', '1179', '3560', '1059', '1634', '4052', '2662', '2774', '1392', '2872', '2872', '1199', '3804', '2955', '2662', '3844', '660', '183', '3383', '2332', '1116', '1370', '764', '1785', '3856', '2737', '3273', '2662', '3273', '716', '3832', '4057', '646', '2340', '1370', '2202', '108', '3772', '4218', '2430', '1221', '3632', '3632']\n",
      "sentence list is:  【 这 事 ， 你 怎 么 看 ？ 】 3 月 2 5 日 晚 ， 杭 州 四 季 青 附 近 ， 一 个 姑 娘 被 一 个 男 人 拖 进 树 丛 。 男 人 抢 过 她 的 拎 包 ， 翻 了 半 天 ， 只 翻 出 一 只 杂 牌 手 机 和 1 2 元 钱 。 “ 你 怎 么 这 么 穷 ？ ” 男 人 一 脸 同 情 ， 然 后 拍 拍 她 肩 膀 ， 豪 气 地 说 ： “ 你 在 这 里 等 我 ， 我 去 抢 点 钱 给 你 打 车 回 家 ！ ” … …\n",
      "sentence label id is: 0\n",
      "---------------------------------\n",
      "2:\n",
      "sentence list id is: ['1269', '1488', '3233', '2340', '1364', '2247', '798', '1179', '1948', '1373', '1369', '3924', '2796', '1878', '2831', '2831', '3341', '2953', '631', '1392', '2662', '2247', '2281', '1176', '2481', '3923', '1347', '3519', '1046', '1608', '1816', '3740', '1637', '2662', '2281', '355', '1402', '2774', '308', '2247', '355', '1222', '609', '2332', '764', '3283', '2481', '2732', '547', '2738', '2410', '1948', '1373', '1369', '1816', '4147', '3925', '2662', '2247', '798', '1830', '3944', '1557', '4229', '1179', '4334', '2662', '4195', '2340', '2247', '798', '1179', '1948', '1373', '1369', '2662', '976', '2823', '1785', '3560', '478', '2558', '652', '1176', '2481', '1422', '3338', '4030', '3560', '2480', '3592', '1488', '2662', '1878', '2831', '2831', '2363', '2953', '2363', '884', '1878', '1369', '694', '2662', '3273', '798', '1730', '51', '2738', '2410', '1948', '1373', '1369', '3776', '2558', '1068', '2480', '3273', '2059', '3484', '1936', '1413', '1523', '743', '3423', '3233', '2208']\n",
      "sentence list is:  【 是 谁 给 了 中 国 人 双 休 日 】   1 9 9 4 年 前 后 ， 中 美 入 世 谈 判 最 艰 难 的 时 刻 ， 美 方 突 然 向 中 方 提 出 ： 在 全 世 界 都 实 行 双 休 日 的 今 天 ， 中 国 必 须 尊 重 人 权 ， 也 给 中 国 人 双 休 日 ， 并 把 这 一 项 作 为 入 世 条 件 之 一 。 于 是 ， 1 9 9 5 年 5 月 1 日 起 ， 我 国 开 始 实 行 双 休 日 工 作 制 。 我 们 到 底 应 该 感 谢 谁 ？\n",
      "sentence label id is: 0\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 打印前2条训练数据\n",
    "vocab = load_vocab(os.path.join(data_root_path, 'dict.txt'))\n",
    "label_list = [\"0\",\"1\"]\n",
    "\n",
    "def ids_to_str(ids):\n",
    "    words = []\n",
    "    for k in ids:\n",
    "        w = list(vocab.keys())[list(vocab.values()).index(int(k))]\n",
    "        words.append(w if isinstance(w, str) else w.decode('ASCII'))\n",
    "    return \" \".join(words)\n",
    "\n",
    "file_path = os.path.join(data_root_path, 'train_list.txt')\n",
    "with io.open(file_path, \"r\", encoding='utf8') as fin:\n",
    "        i = 0\n",
    "        for line in fin:\n",
    "            i += 1\n",
    "            cols = line.strip().split(\"\\t\")\n",
    "            if len(cols) != 2:\n",
    "                sys.stderr.write(\"[NOTICE] Error Format Line!\")\n",
    "                continue\n",
    "            label = int(cols[1])\n",
    "            wids = cols[0].split(\",\")\n",
    "            print(str(i)+\":\")\n",
    "            print('sentence list id is:', wids)\n",
    "            print('sentence list is: ', ids_to_str(wids))\n",
    "            print('sentence label id is:', label)\n",
    "            print('---------------------------------')\n",
    "            \n",
    "            if i == 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-10-15T13:41:40.285840Z",
     "iopub.status.busy": "2022-10-15T13:41:40.285114Z",
     "iopub.status.idle": "2022-10-15T13:41:40.653822Z",
     "shell.execute_reply": "2022-10-15T13:41:40.653173Z",
     "shell.execute_reply.started": "2022-10-15T13:41:40.285813Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============train_dataset =============\n",
      "[1269 1785  184 2662 1370 2351 3181  592 2208 3924 1268  884 4153 2363\n",
      " 1369 2403 2662 1472 3698 1153 2978    9 3648 3658 2662 3560 1522  339\n",
      " 4254  709 3560 1522 2627 1179 3241 3281  323 1431 2480 2627 1179 3832\n",
      " 2287 1199 1816 3182 3243 2662  433 1364 3340 3925 2662 3916  433  609\n",
      " 3560 3916 1933  406 3673 2146 4114 1878 4153 2185  646 2480 1116 1370\n",
      " 2351 3181 1785 3181 3061 2208 1221 2627 1179 3560 1059 1634 4052 2662\n",
      " 2774 1392 2872 2872 1199 3804 2955 2662 3844  660  183 3383 2332 1116\n",
      " 1370  764 1785 3856 2737 3273 2662 3273  716 3832 4057  646 2340 1370\n",
      " 2202  108 3772 4218 2430 1221 3632 3632 4410 4410 4410 4410 4410 4410\n",
      " 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410\n",
      " 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410]\n",
      "(150,)\n",
      "[0]\n",
      "=============test_dataset =============\n",
      "[2354 3963 2332  571 2868  920  427 4147 3925 1876 4025 3762  557 4039\n",
      " 2421 3672  276 1088 2409 1169   23 2461 1229 2332 2247 4226  922  798\n",
      "  631 2764 1465  571 1878 2831 2624 2624 1762 4153  607  607  607 2953\n",
      "  764 1508 2409 2662  798  922 1694 1790 1169 3396 1694 1790 3954 1169\n",
      "  923 1166  995 4030 1889 1816 2868  920  427 1790 4147 3925 1876 4025\n",
      "  764  955 4218 2247 3762  557 4039 2421 3672 3832 1328 1385 3573 3692\n",
      " 1088 3873 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410\n",
      " 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410\n",
      " 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410\n",
      " 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410\n",
      " 4410 4410 4410 4410 4410 4410 4410 4410 4410 4410]\n",
      "(150,)\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "vocab = load_vocab(os.path.join(data_root_path, 'dict.txt'))\n",
    "\n",
    "class RumorDataset(pd.io.Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.all_data = []\n",
    "       \n",
    "        with io.open(self.data_dir, \"r\", encoding='utf8') as fin:\n",
    "            for line in fin:\n",
    "                cols = line.strip().split(\"\\t\")\n",
    "                if len(cols) != 2:\n",
    "                    sys.stderr.write(\"[NOTICE] Error Format Line!\")\n",
    "                    continue\n",
    "                label = []\n",
    "                label.append(int(cols[1]))\n",
    "                wids = cols[0].split(\",\")\n",
    "                if len(wids)>=150:\n",
    "                    wids = np.array(wids[:150]).astype('int64')     \n",
    "                else:\n",
    "                    wids = np.concatenate([wids, [vocab[\"<pad>\"]]*(150-len(wids))]).astype('int64')\n",
    "                label = np.array(label).astype('int64')\n",
    "                self.all_data.append((wids, label))\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data, label = self.all_data[index]\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_data)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = RumorDataset(os.path.join(data_root_path, 'train_list.txt'))\n",
    "test_dataset = RumorDataset(os.path.join(data_root_path, 'eval_list.txt'))\n",
    "\n",
    "train_loader = pd.io.DataLoader(train_dataset, places=pd.CPUPlace(), return_list=True,\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = pd.io.DataLoader(test_dataset, places=pd.CPUPlace(), return_list=True,\n",
    "                                    shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#check\n",
    "\n",
    "print('=============train_dataset =============') \n",
    "for data, label in train_dataset:\n",
    "    print(data)\n",
    "    print(np.array(data).shape)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "\n",
    "print('=============test_dataset =============') \n",
    "for data, label in test_dataset:\n",
    "    print(data)\n",
    "    print(np.array(data).shape)\n",
    "    print(label)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、模型配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T12:44:23.697257Z",
     "iopub.status.busy": "2022-10-15T12:44:23.696151Z",
     "iopub.status.idle": "2022-10-15T12:44:23.702212Z",
     "shell.execute_reply": "2022-10-15T12:44:23.701153Z",
     "shell.execute_reply.started": "2022-10-15T12:44:23.697216Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#参数设置\n",
    "vocab_size = len(vocab)  \n",
    "maxlen = 200  \n",
    "seq_len = 200\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "pad_id = vocab['<pad>']\n",
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "classes = ['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T12:44:26.591679Z",
     "iopub.status.busy": "2022-10-15T12:44:26.590679Z",
     "iopub.status.idle": "2022-10-15T12:44:26.602765Z",
     "shell.execute_reply": "2022-10-15T12:44:26.601748Z",
     "shell.execute_reply.started": "2022-10-15T12:44:26.591632Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key_dense = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value_dense = nn.Linear(embed_dim, embed_dim)\n",
    "        self.combine_heads = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = pd.matmul(query, key, transpose_y=True)\n",
    "        dim_key = pd.cast(pd.shape(key)[-1], 'float32')\n",
    "        scaled_score = score / pd.sqrt(dim_key)\n",
    "        weights = nn.functional.softmax(scaled_score, axis=-1)\n",
    "        output = pd.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        x = pd.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return pd.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = pd.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = pd.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = pd.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T12:44:29.875359Z",
     "iopub.status.busy": "2022-10-15T12:44:29.874356Z",
     "iopub.status.idle": "2022-10-15T12:44:29.880546Z",
     "shell.execute_reply": "2022-10-15T12:44:29.879794Z",
     "shell.execute_reply.started": "2022-10-15T12:44:29.875324Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class PointWiseFeedForwardNetwork(nn.Layer):\n",
    "    def __init__(self, embed_dim, feed_dim):\n",
    "        super(PointWiseFeedForwardNetwork, self).__init__()\n",
    "        self.linear1 = pd.fluid.dygraph.Linear(embed_dim, feed_dim, act='relu')\n",
    "        self.linear2 = nn.Linear(feed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.linear2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T12:44:31.525885Z",
     "iopub.status.busy": "2022-10-15T12:44:31.525145Z",
     "iopub.status.idle": "2022-10-15T12:44:31.530773Z",
     "shell.execute_reply": "2022-10-15T12:44:31.530122Z",
     "shell.execute_reply.started": "2022-10-15T12:44:31.525854Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(nn.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_emb = nn.Embedding(maxlen, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        maxlen = pd.shape(x)[-1]\n",
    "        positions = pd.arange(start=0, end=maxlen, step=1, dtype='int64')\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T12:44:34.000438Z",
     "iopub.status.busy": "2022-10-15T12:44:33.999376Z",
     "iopub.status.idle": "2022-10-15T12:44:34.006938Z",
     "shell.execute_reply": "2022-10-15T12:44:34.006137Z",
     "shell.execute_reply.started": "2022-10-15T12:44:34.000394Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = PointWiseFeedForwardNetwork(embed_dim, feed_dim)\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim, epsilon=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim, epsilon=1e-6)\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attn_output = self.att(inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T12:44:35.951840Z",
     "iopub.status.busy": "2022-10-15T12:44:35.951016Z",
     "iopub.status.idle": "2022-10-15T12:44:35.958737Z",
     "shell.execute_reply": "2022-10-15T12:44:35.957837Z",
     "shell.execute_reply.started": "2022-10-15T12:44:35.951793Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.emb = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "        self.trs = TransformerBlock(embed_dim, num_heads, feed_dim)\n",
    "        self.drop1 = nn.Dropout(0.1)\n",
    "        self.relu = pd.fluid.dygraph.Linear(feed_dim, 20, act='relu')\n",
    "        self.drop2 = nn.Dropout(0.1)\n",
    "        self.soft = pd.fluid.dygraph.Linear(20, 2, act='softmax')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = self.trs(x)\n",
    "        x = pd.mean(x, axis=1)\n",
    "        x = self.drop1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.soft(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T12:50:58.900342Z",
     "iopub.status.busy": "2022-10-15T12:50:58.899307Z",
     "iopub.status.idle": "2022-10-15T12:50:58.904033Z",
     "shell.execute_reply": "2022-10-15T12:50:58.903273Z",
     "shell.execute_reply.started": "2022-10-15T12:50:58.900305Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T13:48:02.705558Z",
     "iopub.status.busy": "2022-10-15T13:48:02.705092Z",
     "iopub.status.idle": "2022-10-15T13:48:05.601149Z",
     "shell.execute_reply": "2022-10-15T13:48:05.600455Z",
     "shell.execute_reply.started": "2022-10-15T13:48:02.705529Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/3\n",
      "step 92/92 [==============================] - loss: 0.6856 - acc: 0.5469 - ETA: 0s - 9ms/ste - loss: 0.6926 - acc: 0.5328 - ETA: 0s - 9ms/ste - loss: 0.6073 - acc: 0.5531 - ETA: 0s - 9ms/ste - loss: 0.5474 - acc: 0.6055 - ETA: 0s - 8ms/ste - loss: 0.4035 - acc: 0.6388 - ETA: 0s - 8ms/ste - loss: 0.5787 - acc: 0.6635 - ETA: 0s - 8ms/ste - loss: 0.4941 - acc: 0.6808 - ETA: 0s - 8ms/ste - loss: 0.5052 - acc: 0.6918 - ETA: 0s - 8ms/ste - loss: 0.5192 - acc: 0.6986 - ETA: 0s - 8ms/ste - loss: 0.5035 - acc: 0.7007 - 8ms/step          \n",
      "save checkpoint at /home/aistudio/model/0\n",
      "Eval begin...\n",
      "step 13/13 [==============================] - loss: 0.4724 - acc: 0.8250 - ETA: 0s - 5ms/ste - loss: 0.4941 - acc: 0.8269 - 5ms/step          \n",
      "Eval samples: 416\n",
      "Epoch 2/3\n",
      "step 92/92 [==============================] - loss: 0.4141 - acc: 0.8750 - ETA: 0s - 8ms/ste - loss: 0.3663 - acc: 0.8812 - ETA: 0s - 8ms/ste - loss: 0.7028 - acc: 0.8500 - ETA: 0s - 8ms/ste - loss: 0.4744 - acc: 0.8562 - ETA: 0s - 8ms/ste - loss: 0.5026 - acc: 0.8456 - ETA: 0s - 8ms/ste - loss: 0.4399 - acc: 0.8469 - ETA: 0s - 8ms/ste - loss: 0.4422 - acc: 0.8509 - ETA: 0s - 8ms/ste - loss: 0.4655 - acc: 0.8496 - ETA: 0s - 8ms/ste - loss: 0.4430 - acc: 0.8497 - ETA: 0s - 8ms/ste - loss: 0.4047 - acc: 0.8495 - 8ms/step          \n",
      "save checkpoint at /home/aistudio/model/1\n",
      "Eval begin...\n",
      "step 13/13 [==============================] - loss: 0.4061 - acc: 0.8531 - ETA: 0s - 5ms/ste - loss: 0.4008 - acc: 0.8582 - 5ms/step          \n",
      "Eval samples: 416\n",
      "Epoch 3/3\n",
      "step 92/92 [==============================] - loss: 0.4314 - acc: 0.8438 - ETA: 0s - 8ms/ste - loss: 0.4615 - acc: 0.8063 - ETA: 0s - 8ms/ste - loss: 0.3703 - acc: 0.8406 - ETA: 0s - 8ms/ste - loss: 0.4856 - acc: 0.8406 - ETA: 0s - 8ms/ste - loss: 0.4102 - acc: 0.8481 - ETA: 0s - 8ms/ste - loss: 0.4435 - acc: 0.8573 - ETA: 0s - 8ms/ste - loss: 0.4022 - acc: 0.8638 - ETA: 0s - 8ms/ste - loss: 0.3879 - acc: 0.8676 - ETA: 0s - 8ms/ste - loss: 0.3277 - acc: 0.8729 - ETA: 0s - 8ms/ste - loss: 0.3950 - acc: 0.8730 - 8ms/step          \n",
      "save checkpoint at /home/aistudio/model/2\n",
      "Eval begin...\n",
      "step 13/13 [==============================] - loss: 0.4436 - acc: 0.8688 - ETA: 0s - 5ms/ste - loss: 0.3867 - acc: 0.8750 - 5ms/step          \n",
      "Eval samples: 416\n",
      "save checkpoint at /home/aistudio/model/final\n",
      "2.8827991485595703\n"
     ]
    }
   ],
   "source": [
    "model = pd.Model(MyNet()) # 用 Model封装 MyNet\n",
    "optim = pd.optimizer.Adam(learning_rate=0.002, parameters=model.parameters())\n",
    "\n",
    "# 配置模型\n",
    "model.prepare(optim,pd.nn.CrossEntropyLoss(),metrics=pd.metric.Accuracy())\n",
    "# # 模型配置\n",
    "# model.prepare(optimizer=pd.optimizer.Adam(learning_rate=0.001, parameters=model.parameters()),\n",
    "#               loss=nn.CrossEntropyLoss(),\n",
    "#               Accuracy())\n",
    "\n",
    "#visualdl=pd.callbacks.VisualDL(log_dir='visual_log')\n",
    "\n",
    "# 模型训练\n",
    "a = time.time()\n",
    "model.fit(train_loader,\n",
    "          test_loader,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          verbose=1,\n",
    "          save_dir='./model/',save_freq=1)\n",
    "b = time.time()\n",
    "print(b-a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T13:48:11.845411Z",
     "iopub.status.busy": "2022-10-15T13:48:11.844591Z",
     "iopub.status.idle": "2022-10-15T13:48:11.920910Z",
     "shell.execute_reply": "2022-10-15T13:48:11.920276Z",
     "shell.execute_reply.started": "2022-10-15T13:48:11.845376Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict begin...\n",
      "step 14/14 [==============================] - ETA: 0s - 5ms/st - ETA: 0s - 5ms/st - ETA: 0s - 5ms/st - ETA: 0s - 5ms/st - ETA: 0s - 5ms/st - ETA: 0s - 4ms/st - 4ms/step          \n",
      "Predict samples: 424\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[155,  38],\n",
       "       [ 14, 217]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "result = model.predict(test_dataset, batch_size=32, num_workers=0, stack_outputs=True, callbacks=None)\n",
    "\n",
    "#print(result)\n",
    "R=result[0]\n",
    "P=np.argmax(R, axis=1)\n",
    "print\n",
    "r = []\n",
    "for data, label in test_dataset:\n",
    "    r.append(label)\n",
    "R = r\n",
    "confusion_matrix(R, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-15T13:48:14.569244Z",
     "iopub.status.busy": "2022-10-15T13:48:14.568656Z",
     "iopub.status.idle": "2022-10-15T13:48:14.578576Z",
     "shell.execute_reply": "2022-10-15T13:48:14.577939Z",
     "shell.execute_reply.started": "2022-10-15T13:48:14.569217Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.80      0.86       193\n",
      "           1       0.85      0.94      0.89       231\n",
      "\n",
      "    accuracy                           0.88       424\n",
      "   macro avg       0.88      0.87      0.87       424\n",
      "weighted avg       0.88      0.88      0.88       424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"0\",\"1\"]\n",
    "CR=classification_report(R, P, target_names=target_names)\n",
    "print(CR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "import paddle\n",
    "from paddle.nn import Conv2D, Linear, Embedding\n",
    "from paddle import to_tensor\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "class RNN(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.dict_dim = vocab[\"<pad>\"]\n",
    "        self.emb_dim = 128\n",
    "        self.hid_dim = 128\n",
    "        self.class_dim = 2\n",
    "        self.embedding = Embedding(\n",
    "            self.dict_dim + 1, self.emb_dim,\n",
    "            sparse=False)\n",
    "        self._fc1 = Linear(self.emb_dim, self.hid_dim)\n",
    "        self.lstm = paddle.nn.LSTM(self.hid_dim, self.hid_dim)\n",
    "        self.fc2 = Linear(19200, self.class_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # [32, 150]\n",
    "        emb = self.embedding(inputs)\n",
    "        # [32, 150, 128]\n",
    "        fc_1 = self._fc1(emb)\n",
    "        # [32, 150, 128]\n",
    "        x = self.lstm(fc_1)\n",
    "        x = paddle.reshape(x[0], [0, -1])\n",
    "        x = self.fc2(x)\n",
    "        x = paddle.nn.functional.softmax(x)\n",
    "        return x\n",
    "\n",
    "rnn = RNN()\n",
    "paddle.summary(rnn,(32,150),\"int64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def draw_process(title,color,iters,data,label):\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"iter\", fontsize=20)\n",
    "    plt.ylabel(label, fontsize=20)\n",
    "    plt.plot(iters, data,color=color,label=label) \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "def train(model):\n",
    "    model.train()\n",
    "    opt = pd.optimizer.Adam(learning_rate=0.002, parameters=model.parameters())\n",
    "    \n",
    "    steps = 0\n",
    "    Iters, total_loss, total_acc = [], [], []\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        for batch_id, data in enumerate(train_loader):\n",
    "            steps += 1\n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "            \n",
    "            logits = model(sent)\n",
    "            loss = pd.nn.functional.cross_entropy(logits, label)\n",
    "            acc = pd.metric.accuracy(logits, label)\n",
    "            recall = pd.metric.Recall()\n",
    "            precision = pd.metric.Precision()\n",
    "\n",
    "            if batch_id % 50 == 0:\n",
    "                Iters.append(steps)\n",
    "                total_loss.append(loss.numpy()[0])\n",
    "                total_acc.append(acc.numpy()[0])\n",
    "\n",
    "                print(\"epoch: {}, batch_id: {}, loss is: {}\".format(epoch, batch_id, loss.numpy()))\n",
    "            \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.clear_grad()\n",
    "\n",
    "        # evaluate model after one epoch\n",
    "        model.eval()\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        \n",
    "        for batch_id, data in enumerate(test_loader):\n",
    "            \n",
    "            sent = data[0]\n",
    "            label = data[1]\n",
    "\n",
    "            logits = model(sent)\n",
    "            loss = pd.nn.functional.cross_entropy(logits, label)\n",
    "            acc = pd.metric.accuracy(logits, label)\n",
    "            \n",
    "            accuracies.append(acc.numpy())\n",
    "            losses.append(loss.numpy())\n",
    "        \n",
    "        avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "\n",
    "        print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "    pd.save(model.state_dict(),\"model_final.pdparams\")\n",
    "    \n",
    "    draw_process(\"trainning loss\",\"red\",Iters,total_loss,\"trainning loss\")\n",
    "    draw_process(\"trainning acc\",\"green\",Iters,total_acc,\"trainning acc\")\n",
    "        \n",
    "import time\n",
    "start_time=time.time()\n",
    "model = MyNet()\n",
    "train(model)\n",
    "end_time=time.time()\n",
    "running_time=end_time-start_time\n",
    "print(running_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T14:41:01.150625Z",
     "iopub.status.busy": "2022-09-23T14:41:01.149470Z",
     "iopub.status.idle": "2022-09-23T14:41:01.304250Z",
     "shell.execute_reply": "2022-09-23T14:41:01.303195Z",
     "shell.execute_reply.started": "2022-09-23T14:41:01.150585Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[validation] accuracy: 0.838942289352417, loss: 0.46633583307266235\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "模型评估\n",
    "'''\n",
    "model_state_dict = pd.load('/home/aistudio/model/final.pdparams')\n",
    "model = MyNet()\n",
    "model.set_state_dict(model_state_dict) \n",
    "model.eval()\n",
    "accuracies = []\n",
    "losses = []\n",
    "recalles = []\n",
    "precisiones = []\n",
    "\n",
    "for batch_id, data in enumerate(test_loader):\n",
    "    \n",
    "    sent = data[0]\n",
    "    label = data[1]\n",
    "\n",
    "    logits = model(sent)\n",
    "    loss = pd.nn.functional.cross_entropy(logits, label)\n",
    "    acc = pd.metric.accuracy(logits, label)\n",
    "\n",
    "    \n",
    "    accuracies.append(acc.numpy())\n",
    "    losses.append(loss.numpy())\n",
    "    \n",
    "\n",
    "avg_acc, avg_loss = np.mean(accuracies), np.mean(losses)\n",
    "print(\"[validation] accuracy: {}, loss: {}\".format(avg_acc, avg_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-09-23T14:39:14.476436Z",
     "iopub.status.busy": "2022-09-23T14:39:14.475292Z",
     "iopub.status.idle": "2022-09-23T14:39:14.544613Z",
     "shell.execute_reply": "2022-09-23T14:39:14.543803Z",
     "shell.execute_reply.started": "2022-09-23T14:39:14.476393Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "数据: 【 国 家 工 商 总 局 决 定 停 止 企 业 年 检 】 中 国 国 家 工 商 总 局 决 定 ， 自 今 年 3 月 1 日 起 停 止 对 领 取 营 业 执 照 的 有 限 责 任 公 司 、 股 份 有 限 公 司 、 非 公 司 企 业 法 人 、 合 伙 企 业 、 个 人 独 资 企 业 及 其 分 支 机 构 、 来 华 从 事 经 营 活 动 的 外 国 （ 地 区 ） 企 业 ， 及 其 他 经 营 单 位 的 企 业 年 检 工 作 。 新 华 网 h t t p : / / t . c n / 8 F H w G 7 9   h t t p : / / t . \n",
      "\n",
      "是否谣言: 否\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "label_map = {0:\"是\", 1:\"否\"}\n",
    "\n",
    "model_state_dict = pd.load('/home/aistudio/model/final.pdparams')\n",
    "model = MyNet()\n",
    "model.set_state_dict(model_state_dict) \n",
    "model.eval()\n",
    "\n",
    "for batch_id, data in enumerate(test_loader):\n",
    "    \n",
    "    sent = data[0]\n",
    "    results = model(sent)\n",
    "\n",
    "    predictions = []\n",
    "    for probs in results:\n",
    "        # 映射分类label\n",
    "        idx = np.argmax(probs)\n",
    "        labels = label_map[idx]\n",
    "        predictions.append(labels)\n",
    "    \n",
    "    for i,pre in enumerate(predictions):\n",
    "        print('数据: {} \\n\\n是否谣言: {}'.format(ids_to_str(sent[0]), pre))\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
