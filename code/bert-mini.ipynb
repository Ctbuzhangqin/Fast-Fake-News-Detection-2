{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:09:28.751834Z",
     "iopub.status.busy": "2023-01-10T08:09:28.750454Z",
     "iopub.status.idle": "2023-01-10T08:09:36.639189Z",
     "shell.execute_reply": "2023-01-10T08:09:36.638011Z",
     "shell.execute_reply.started": "2023-01-10T08:09:28.751769Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting paddlenlp==2.0.0b4\r\n",
      "  Downloading paddlenlp-2.0.0b4-py3-none-any.whl (164 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.3/164.3 kB\u001b[0m \u001b[31m104.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (2.9.0)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (0.4.4)\r\n",
      "Requirement already satisfied: seqeval in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (1.2.2)\r\n",
      "Requirement already satisfied: jieba in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (0.42.1)\r\n",
      "Requirement already satisfied: visualdl in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (2.2.0)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlenlp==2.0.0b4) (4.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.7 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp==2.0.0b4) (1.20.3)\r\n",
      "Requirement already satisfied: six in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from h5py->paddlenlp==2.0.0b4) (1.16.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from seqeval->paddlenlp==2.0.0b4) (0.24.2)\r\n",
      "Requirement already satisfied: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (1.21.0)\r\n",
      "Requirement already satisfied: flask>=1.1.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (1.1.1)\r\n",
      "Requirement already satisfied: flake8>=3.7.9 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (4.0.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (2.22.0)\r\n",
      "Requirement already satisfied: bce-python-sdk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (0.8.53)\r\n",
      "Requirement already satisfied: Flask-Babel>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (1.0.0)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (2.2.3)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (1.1.5)\r\n",
      "Requirement already satisfied: shellcheck-py in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (0.7.1.1)\r\n",
      "Requirement already satisfied: Pillow>=7.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (7.1.2)\r\n",
      "Requirement already satisfied: protobuf>=3.11.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from visualdl->paddlenlp==2.0.0b4) (3.20.1)\r\n",
      "Requirement already satisfied: importlib-metadata<4.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.0b4) (4.2.0)\r\n",
      "Requirement already satisfied: pycodestyle<2.9.0,>=2.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.0b4) (2.8.0)\r\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.0b4) (0.6.1)\r\n",
      "Requirement already satisfied: pyflakes<2.5.0,>=2.4.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8>=3.7.9->visualdl->paddlenlp==2.0.0b4) (2.4.0)\r\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.0b4) (3.0.0)\r\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.0b4) (1.1.0)\r\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.0b4) (0.16.0)\r\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.1->visualdl->paddlenlp==2.0.0b4) (8.0.4)\r\n",
      "Requirement already satisfied: pytz in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.0.0b4) (2019.3)\r\n",
      "Requirement already satisfied: Babel>=2.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Flask-Babel>=1.0.0->visualdl->paddlenlp==2.0.0b4) (2.8.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.0b4) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.0b4) (0.14.1)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from scikit-learn>=0.21.3->seqeval->paddlenlp==2.0.0b4) (1.6.3)\r\n",
      "Requirement already satisfied: pycryptodome>=3.8.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp==2.0.0b4) (3.9.9)\r\n",
      "Requirement already satisfied: future>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from bce-python-sdk->visualdl->paddlenlp==2.0.0b4) (0.18.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.0.0b4) (1.1.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.0.0b4) (2.8.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.0.0b4) (0.10.0)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from matplotlib->visualdl->paddlenlp==2.0.0b4) (3.0.9)\r\n",
      "Requirement already satisfied: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (1.3.0)\r\n",
      "Requirement already satisfied: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (16.7.9)\r\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (1.3.4)\r\n",
      "Requirement already satisfied: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (0.10.0)\r\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (1.4.10)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (5.1.2)\r\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->visualdl->paddlenlp==2.0.0b4) (2.0.1)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.0b4) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.0b4) (2019.9.11)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.0b4) (2.8)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->visualdl->paddlenlp==2.0.0b4) (1.25.6)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata<4.3->flake8>=3.7.9->visualdl->paddlenlp==2.0.0b4) (4.3.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata<4.3->flake8>=3.7.9->visualdl->paddlenlp==2.0.0b4) (3.8.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.1->visualdl->paddlenlp==2.0.0b4) (2.0.1)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->visualdl->paddlenlp==2.0.0b4) (56.2.0)\r\n",
      "Installing collected packages: paddlenlp\r\n",
      "  Attempting uninstall: paddlenlp\r\n",
      "    Found existing installation: paddlenlp 2.0.7\r\n",
      "    Uninstalling paddlenlp-2.0.7:\r\n",
      "      Successfully uninstalled paddlenlp-2.0.7\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "paddlehub 2.0.4 requires paddlenlp>=2.0.0rc5, but you have paddlenlp 2.0.0b4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed paddlenlp-2.0.0b4\r\n",
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade paddlenlp==2.0.0b4 -i https://pypi.org/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:11:16.020258Z",
     "iopub.status.busy": "2023-01-10T08:11:16.019614Z",
     "iopub.status.idle": "2023-01-10T08:11:16.029435Z",
     "shell.execute_reply": "2023-01-10T08:11:16.028684Z",
     "shell.execute_reply.started": "2023-01-10T08:11:16.020211Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.2\r\n"
     ]
    }
   ],
   "source": [
    "import re\r\n",
    "import jieba\r\n",
    "import os \r\n",
    "import random\r\n",
    "import paddle\r\n",
    "import paddlenlp as ppnlp\r\n",
    "from paddlenlp.data import Stack, Pad, Tuple\r\n",
    "import paddle.nn.functional as F\r\n",
    "import paddle.nn as nn\r\n",
    "from visualdl import LogWriter\r\n",
    "import numpy as np\r\n",
    "from functools import partial #partial()函数可以用来固定某些参数值，并返回一个新的callable对象\r\n",
    "import os, zipfile\r\n",
    "import io, random, json\r\n",
    "print(paddle.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:10:43.652377Z",
     "iopub.status.busy": "2023-01-10T08:10:43.651707Z",
     "iopub.status.idle": "2023-01-10T08:10:46.711149Z",
     "shell.execute_reply": "2023-01-10T08:10:46.710224Z",
     "shell.execute_reply.started": "2023-01-10T08:10:43.652332Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#解压原始数据集，将Rumor_Dataset.zip解压至data目录下\r\n",
    "src_path=\"data/data20519/Rumor_Dataset.zip\"\r\n",
    "target_path=\"/home/aistudio/data/Chinese_Rumor_Dataset-master\"\r\n",
    "if(not os.path.isdir(target_path)):\r\n",
    "    z = zipfile.ZipFile(src_path, 'r')\r\n",
    "    z.extractall(path=target_path)\r\n",
    "    z.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:11:18.856827Z",
     "iopub.status.busy": "2023-01-10T08:11:18.856144Z",
     "iopub.status.idle": "2023-01-10T08:11:19.474742Z",
     "shell.execute_reply": "2023-01-10T08:11:19.473790Z",
     "shell.execute_reply.started": "2023-01-10T08:11:18.856781Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#数据文件路径\r\n",
    "rumor_class_dirs = os.listdir(target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/rumor-repost/\")\r\n",
    "\r\n",
    "non_rumor_class_dirs = os.listdir(target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/non-rumor-repost/\")\r\n",
    "original_microblog = target_path+\"/Chinese_Rumor_Dataset-master/CED_Dataset/original-microblog/\"\r\n",
    "\r\n",
    "rumor_label=\"0\"\r\n",
    "non_rumor_label=\"1\"\r\n",
    "\r\n",
    "\r\n",
    "rumor_num = 0\r\n",
    "non_rumor_num = 0\r\n",
    "all_rumor_list = []\r\n",
    "all_non_rumor_list = []\r\n",
    "\r\n",
    "#解析数据\r\n",
    "for rumor_class_dir in rumor_class_dirs: \r\n",
    "    if(rumor_class_dir != '.DS_Store'):\r\n",
    "        #遍历数据，并解析\r\n",
    "        with open(original_microblog + rumor_class_dir, 'r') as f:\r\n",
    "\t        rumor_content = f.read()\r\n",
    "        rumor_dict = json.loads(rumor_content)\r\n",
    "        all_rumor_list.append(rumor_label+\"\\t\"+rumor_dict[\"text\"]+\"\\n\")\r\n",
    "        rumor_num +=1\r\n",
    "\r\n",
    "#解析数据\r\n",
    "for non_rumor_class_dir in non_rumor_class_dirs: \r\n",
    "    if(non_rumor_class_dir != '.DS_Store'):\r\n",
    "        with open(original_microblog + non_rumor_class_dir, 'r') as f2:\r\n",
    "\t        non_rumor_content = f2.read()\r\n",
    "        non_rumor_dict = json.loads(non_rumor_content)\r\n",
    "        all_non_rumor_list.append(non_rumor_label+\"\\t\"+non_rumor_dict[\"text\"]+\"\\n\")\r\n",
    "        non_rumor_num +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:11:29.661580Z",
     "iopub.status.busy": "2023-01-10T08:11:29.660847Z",
     "iopub.status.idle": "2023-01-10T08:11:29.675016Z",
     "shell.execute_reply": "2023-01-10T08:11:29.674161Z",
     "shell.execute_reply.started": "2023-01-10T08:11:29.661521Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_list_path=\"/home/aistudio/data/\"\r\n",
    "all_data_path=data_list_path + \"all_data.txt\"\r\n",
    "\r\n",
    "all_data_list = all_rumor_list + all_non_rumor_list\r\n",
    "\r\n",
    "random.shuffle(all_data_list)\r\n",
    "\r\n",
    "#在生成all_data.txt之前，首先将其清空\r\n",
    "with open(all_data_path, 'w') as f:\r\n",
    "    f.seek(0)\r\n",
    "    f.truncate() \r\n",
    "    \r\n",
    "with open(all_data_path, 'a') as f:\r\n",
    "    for data in all_data_list:\r\n",
    "        f.write(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:11:41.018334Z",
     "iopub.status.busy": "2023-01-10T08:11:41.017871Z",
     "iopub.status.idle": "2023-01-10T08:11:41.026124Z",
     "shell.execute_reply": "2023-01-10T08:11:41.025301Z",
     "shell.execute_reply.started": "2023-01-10T08:11:41.018298Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 生成数据字典\r\n",
    "def create_dict(data_path, dict_path):\r\n",
    "    with open(dict_path, 'w') as f:\r\n",
    "        f.seek(0)\r\n",
    "        f.truncate() \r\n",
    "\r\n",
    "    dict_set = set()\r\n",
    "    # 读取全部数据\r\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\r\n",
    "        lines = f.readlines()\r\n",
    "    # 把数据生成一个元组\r\n",
    "    for line in lines:\r\n",
    "        content = line.split('\\t')[-1].replace('\\n', '')\r\n",
    "        for s in content:\r\n",
    "            dict_set.add(s)\r\n",
    "    # 把元组转换成字典，一个字对应一个数字\r\n",
    "    dict_list = []\r\n",
    "    i = 0\r\n",
    "    for s in dict_set:\r\n",
    "        dict_list.append([s, i])\r\n",
    "        i += 1\r\n",
    "    # 添加未知字符\r\n",
    "    dict_txt = dict(dict_list)\r\n",
    "    end_dict = {\"<unk>\": i}\r\n",
    "    dict_txt.update(end_dict)\r\n",
    "    end_dict = {\"<pad>\": i+1}\r\n",
    "    dict_txt.update(end_dict)\r\n",
    "    # 把这些字典保存到本地中\r\n",
    "    with open(dict_path, 'w', encoding='utf-8') as f:\r\n",
    "        f.write(str(dict_txt))\r\n",
    "        \r\n",
    "    print(\"数据字典生成完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:11:53.682972Z",
     "iopub.status.busy": "2023-01-10T08:11:53.682075Z",
     "iopub.status.idle": "2023-01-10T08:11:53.692166Z",
     "shell.execute_reply": "2023-01-10T08:11:53.691372Z",
     "shell.execute_reply.started": "2023-01-10T08:11:53.682928Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_vocab(file_path):\r\n",
    "    fr = open(file_path, 'r', encoding='utf8')\r\n",
    "    vocab = eval(fr.read())   #读取的str转换为字典\r\n",
    "    fr.close()\r\n",
    "\r\n",
    "    return vocab\r\n",
    "\r\n",
    "def f_write_txt(words, dict_txt, label):\r\n",
    "    labs = \"\"\r\n",
    "    for s in words:\r\n",
    "        lab = str(dict_txt[s])\r\n",
    "        labs = labs + lab + ','\r\n",
    "    labs = labs[:-1]\r\n",
    "    labs = labs + '\\t' + label + '\\n'\r\n",
    "    return labs\r\n",
    "\r\n",
    "def create_data_list(data_path, train_path, test_path, eval_path, dict_path):\r\n",
    "    \r\n",
    "    dict_txt = load_vocab(dict_path)\r\n",
    "    with open(data_path, 'r', encoding='utf-8') as f_data:\r\n",
    "        lines = f_data.readlines()\r\n",
    "\r\n",
    "    i = 0\r\n",
    "    maxlen = 0\r\n",
    "    with open(test_path, 'a', encoding='utf-8') as f_test,open(train_path, 'a', encoding='utf-8') as f_train, open(eval_path, 'a', encoding='utf-8') as f_eval:\r\n",
    "        for line in lines:\r\n",
    "            words = line.split('\\t')[-1].replace('\\n', '')\r\n",
    "            maxlen = max(maxlen, len(words))\r\n",
    "            label = line.split('\\t')[0]\r\n",
    "            labs = f_write_txt(words, dict_txt, label)\r\n",
    "            # 每8个 抽取一个数据用于验证\r\n",
    "            if i % 8 == 0:\r\n",
    "                f_test.write(labs)\r\n",
    "            if i % 5 == 0:\r\n",
    "                f_eval.write(labs)\r\n",
    "            else:\r\n",
    "                f_train.write(labs)\r\n",
    "            i += 1\r\n",
    "    print(\"数据列表生成完成！\")\r\n",
    "    print(maxlen)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:12:06.739492Z",
     "iopub.status.busy": "2023-01-10T08:12:06.738819Z",
     "iopub.status.idle": "2023-01-10T08:12:07.017444Z",
     "shell.execute_reply": "2023-01-10T08:12:07.016640Z",
     "shell.execute_reply.started": "2023-01-10T08:12:06.739448Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据字典生成完成！\r\n",
      "数据列表生成完成！\r\n",
      "226\r\n"
     ]
    }
   ],
   "source": [
    "data_root_path = \"/home/aistudio/data/\" \r\n",
    "data_path = os.path.join(data_root_path, 'all_data.txt')\r\n",
    "train_path = os.path.join(data_list_path, 'train_list.txt')\r\n",
    "test_path = os.path.join(data_list_path, 'test_list.txt')\r\n",
    "eval_path = os.path.join(data_list_path, 'eval_list.txt')\r\n",
    "dict_path = os.path.join(data_root_path, \"dict.txt\")\r\n",
    "\r\n",
    "# 创建数据字典\r\n",
    "create_dict(data_path, dict_path)\r\n",
    "\r\n",
    "# 创建数据列表\r\n",
    "\r\n",
    "#在生成数据之前，首先将eval_list.txt和train_list.txt清空\r\n",
    "with open(test_path, 'w', encoding='utf-8') as f_test:\r\n",
    "    f_test.seek(0)\r\n",
    "    f_test.truncate()\r\n",
    "with open(train_path, 'w', encoding='utf-8') as f_train:\r\n",
    "    f_train.seek(0)\r\n",
    "    f_train.truncate() \r\n",
    "with open(eval_path, 'w', encoding='utf-8') as f_eval:\r\n",
    "    f_eval.seek(0)\r\n",
    "    f_eval.truncate() \r\n",
    "\r\n",
    "create_data_list(data_path, train_path, test_path ,eval_path, dict_path)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:12:51.526128Z",
     "iopub.status.busy": "2023-01-10T08:12:51.524966Z",
     "iopub.status.idle": "2023-01-10T08:12:51.549139Z",
     "shell.execute_reply": "2023-01-10T08:12:51.548212Z",
     "shell.execute_reply.started": "2023-01-10T08:12:51.526083Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 打印前2条训练数据\r\n",
    "vocab = load_vocab(dict_path)\r\n",
    "\r\n",
    "def ids_to_str(ids):\r\n",
    "    words = []\r\n",
    "    for k in ids:\r\n",
    "        w = list(vocab.keys())[list(vocab.values()).index(int(k))]\r\n",
    "        words.append(w if isinstance(w, str) else w.decode('ASCII'))\r\n",
    "    return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:12:53.175190Z",
     "iopub.status.busy": "2023-01-10T08:12:53.174552Z",
     "iopub.status.idle": "2023-01-10T08:12:58.548250Z",
     "shell.execute_reply": "2023-01-10T08:12:58.547292Z",
     "shell.execute_reply.started": "2023-01-10T08:12:53.175147Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f=open(\"eval_list.txt\",\"w\")\r\n",
    "\r\n",
    "with io.open(eval_path, \"r\", encoding='utf8') as fin:\r\n",
    "        i = 0\r\n",
    "        for line in fin:\r\n",
    "            i += 1\r\n",
    "            cols = line.strip().split(\"\\t\")\r\n",
    "            if len(cols) != 2:\r\n",
    "                sys.stderr.write(\"[NOTICE] Error Format Line!\")\r\n",
    "                continue\r\n",
    "            label = int(cols[1])\r\n",
    "            wids = cols[0].split(\",\")\r\n",
    "            strline = ids_to_str(wids) + \"\\t\" + str(label) + \"\\n\"\r\n",
    "            f.writelines(strline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:13:34.939669Z",
     "iopub.status.busy": "2023-01-10T08:13:34.938459Z",
     "iopub.status.idle": "2023-01-10T08:13:56.727658Z",
     "shell.execute_reply": "2023-01-10T08:13:56.726603Z",
     "shell.execute_reply.started": "2023-01-10T08:13:34.939613Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f=open(\"train.txt\",\"w\")\r\n",
    "\r\n",
    "with io.open(train_path, \"r\", encoding='utf8') as fin:\r\n",
    "        i = 0\r\n",
    "        for line in fin:\r\n",
    "            i += 1\r\n",
    "            cols = line.strip().split(\"\\t\")\r\n",
    "            if len(cols) != 2:\r\n",
    "                sys.stderr.write(\"[NOTICE] Error Format Line!\")\r\n",
    "                continue\r\n",
    "            label = int(cols[1])\r\n",
    "            wids = cols[0].split(\",\")\r\n",
    "            strline = ids_to_str(wids) + \"\\t\" + str(label) + \"\\n\"\r\n",
    "            f.writelines(strline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:14:12.963535Z",
     "iopub.status.busy": "2023-01-10T08:14:12.962381Z",
     "iopub.status.idle": "2023-01-10T08:14:16.455887Z",
     "shell.execute_reply": "2023-01-10T08:14:16.454895Z",
     "shell.execute_reply.started": "2023-01-10T08:14:12.963485Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f=open(\"test_list.txt\",\"w\")\r\n",
    "\r\n",
    "with io.open(test_path, \"r\", encoding='utf8') as fin:\r\n",
    "        i = 0\r\n",
    "        for line in fin:\r\n",
    "            i += 1\r\n",
    "            cols = line.strip().split(\"\\t\")\r\n",
    "            if len(cols) != 2:\r\n",
    "                sys.stderr.write(\"[NOTICE] Error Format Line!\")\r\n",
    "                continue\r\n",
    "            label = int(cols[1])\r\n",
    "            wids = cols[0].split(\",\")\r\n",
    "            strline = ids_to_str(wids) + \"\\t\" + str(label) + \"\\n\"\r\n",
    "            f.writelines(strline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:16:46.376700Z",
     "iopub.status.busy": "2023-01-10T08:16:46.376056Z",
     "iopub.status.idle": "2023-01-10T08:16:46.394947Z",
     "shell.execute_reply": "2023-01-10T08:16:46.394155Z",
     "shell.execute_reply.started": "2023-01-10T08:16:46.376654Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#数据处理已经做好可以从这里开始运行代码\r\n",
    "class SelfDefinedDataset(paddle.io.Dataset):\r\n",
    "    def __init__(self, data):\r\n",
    "        super(SelfDefinedDataset, self).__init__()\r\n",
    "        self.data = data\r\n",
    "\r\n",
    "    def __getitem__(self, idx):\r\n",
    "        return self.data[idx]\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.data)\r\n",
    "        \r\n",
    "    def get_labels(self):\r\n",
    "        return [\"0\", \"1\"]\r\n",
    "\r\n",
    "def txt_to_list(file_name):\r\n",
    "    res_list = []\r\n",
    "    for line in open(file_name):\r\n",
    "        res_list.append(line.strip().split('\\t'))\r\n",
    "    return res_list\r\n",
    "\r\n",
    "trainlst = txt_to_list('train.txt')\r\n",
    "devlst = txt_to_list('eval_list.txt')\r\n",
    "testlst = txt_to_list('test_list.txt')\r\n",
    "\r\n",
    "train_ds, dev_ds, test_ds = SelfDefinedDataset.get_datasets([trainlst, devlst, testlst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:17:31.045585Z",
     "iopub.status.busy": "2023-01-10T08:17:31.044443Z",
     "iopub.status.idle": "2023-01-10T08:17:31.059890Z",
     "shell.execute_reply": "2023-01-10T08:17:31.059128Z",
     "shell.execute_reply.started": "2023-01-10T08:17:31.045531Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据：[['有 人 说 ， 失 眠 ， 是 因 为 你 正 在 别 人 的 梦 里 辛 苦 地 忙 碌 着 。     晚 安', '1'], ['# 自 然 堂 全 国 路 演 随 手 拍 # 自 然 堂 舞 林 争 霸 冠 军 果 然 是 我 喜 欢 的 @ 张 傲 月   ， 一 路 走 来 一 直 都 非 常 支 持 他 ， 没 想 到 六 一 儿 童 节 还 在 王 府 井 东 安 市 场 看 到 他 参 加 的 自 然 堂 路 演 活 动 ， 在 现 场 看 见 了 他 真 人 跳 舞 哦 ， 喜 欢 张 傲 月 的 女 粉 们 ， 赶 紧 来 收 图 啦 ！ 在 现 场 我 还 体 验 了 自 然 堂 的 产 品 ， 更 加 期 待 惊 喜 大 奖 了 ~', '1'], ['[ 转 ]   有 车 的 朋 友 要 注 意 了 啊 ！ ！ ！ 加 油 时 你 要 注 意 加 油 员 的 这 个 小 动 作 了 . . .', '0']]\r\n",
      "\r\n",
      "验证集数据:[['【 拾 金 不 昧 要 慎 重 】 长 沙 一 小 孩 上 学 路 上 捡 到 3 万 元 ， 就 坐 在 那 里 等 失 主 ， 结 果 被 人 冒 领 ， 小 孩 回 校 告 知 老 师 后 得 到 表 扬 ， 不 久 真 失 主 听 说 小 孩 拾 金 不 昧 的 事 后 找 上 门 索 钱 ， 未 得 ， 就 告 上 法 庭 ， 最 后 判 小 孩 家 长 赔 2 0 % 即 6 0 0 0 元 。 这 是 真 实 事 件 ， 请 大 家 以 此 为 鉴 ， 教 育 小 孩 拾 金 不 昧 要 慎 重 啊 。 （ 转 ）', '0'], ['1 3 1 2 3 1   # 东 方 卫 视 跨 年 倒 计 时 #   # 吴 亦 凡 #   # k r i s #   天 生 丽 质 难 自 弃 、 含 情 凝 睇 谢 君 王 、 回 眸 一 笑 百 媚 生 、 此 恨 绵 绵 无 绝 期   [ 挖 鼻 屎 ]   长 恨 歌 那 么 长 当 年 是 怎 么 背 下 来 的 [ 偷 笑 ]   第 一 张 图 我 打 赌 你 无 法 盯 着 超 过 1 0 秒 . . . . . . . 这 张 是 个 宝 ， 谁 偷 我 跟 谁 吵 ！', '1'], ['2 0 1 3 ， 我 们 如 智 者 ， 面 对 生 活 乐 观 积 极 ， 我 们 似 行 者 ， 追 求 梦 想 永 不 停 歇 ， 我 们 做 勇 者 ， 面 对 挫 折 越 挫 越 勇 ， 我 们 为 德 者 ， 化 解 仇 恨 以 德 报 怨 ， 这 一 年 我 们 收 获 了 很 多 ， 愿 2 0 1 4 可 以 更 加 美 好 幸 福 。 @ 鲜 花 的 快 乐 生 活   专 业 提 供 网 上 订 花 、 鲜 花 速 递 ， 市 区 最 快 三 小 时 内 送 达 ！   h t t p : / / t . c n / 8 k x 5 M 5 J', '1']]\r\n",
      "\r\n",
      "测试集数据:[['【 拾 金 不 昧 要 慎 重 】 长 沙 一 小 孩 上 学 路 上 捡 到 3 万 元 ， 就 坐 在 那 里 等 失 主 ， 结 果 被 人 冒 领 ， 小 孩 回 校 告 知 老 师 后 得 到 表 扬 ， 不 久 真 失 主 听 说 小 孩 拾 金 不 昧 的 事 后 找 上 门 索 钱 ， 未 得 ， 就 告 上 法 庭 ， 最 后 判 小 孩 家 长 赔 2 0 % 即 6 0 0 0 元 。 这 是 真 实 事 件 ， 请 大 家 以 此 为 鉴 ， 教 育 小 孩 拾 金 不 昧 要 慎 重 啊 。 （ 转 ）', '0'], ['今 天 听 到 邻 居 家 小 萝 莉 唱 大 风 车 ， 插 电 没 把 我 憋 成 内 伤 ， 她 是 这 样 唱 的 ： “ 大 风 车 呀 直 呦 呦 转 ， 他 妈 滴 ， 他 妈 滴 真 好 看 . . . ”     ` 转', '1'], ['【 人 民 日 报 刊 文 称 房 价 过 高 上 涨 过 快 阻 碍 城 镇 化 】 人 民 日 报 今 日 刊 文 称 ， 城 镇 化 的 加 快 推 进 ， 必 然 会 扩 大 对 城 镇 商 品 住 宅 需 求 。 2 0 0 5 年 以 来 ， 我 国 房 价 过 高 、 上 涨 过 快 ， 提 高 了 城 镇 化 成 本 ， 阻 碍 了 城 镇 化 进 程 ； 建 议 适 时 开 征 房 产 税 ， 并 实 行 差 额 累 进 税 率 ， 用 税 收 杠 杆 抑 制 房 地 产 投 机 。   详 见 ： h t t p : / / t . c n / z Y w 3 r E N', '1']]\r\n",
      "\r\n",
      "训练集样本个数:2709\r\n",
      "验证集样本个数:678\r\n",
      "测试集样本个数:408\r\n"
     ]
    }
   ],
   "source": [
    "label_list = train_ds.get_labels()\r\n",
    "#看看数据长什么样子，分别打印训练集、验证集、测试集的前3条数据。\r\n",
    "print(\"训练集数据：{}\\n\".format(train_ds[0:3]))\r\n",
    "print(\"验证集数据:{}\\n\".format(dev_ds[0:3]))\r\n",
    "print(\"测试集数据:{}\\n\".format(test_ds[0:3]))\r\n",
    "\r\n",
    "print(\"训练集样本个数:{}\".format(len(train_ds)))\r\n",
    "print(\"验证集样本个数:{}\".format(len(dev_ds)))\r\n",
    "print(\"测试集样本个数:{}\".format(len(test_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:16:59.942654Z",
     "iopub.status.busy": "2023-01-10T08:16:59.941982Z",
     "iopub.status.idle": "2023-01-10T08:16:59.948303Z",
     "shell.execute_reply": "2023-01-10T08:16:59.947524Z",
     "shell.execute_reply.started": "2023-01-10T08:16:59.942612Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#验证清洗脏数据\r\n",
    "for line in train_ds:\r\n",
    "    if len(line) != 2:\r\n",
    "        print(\"error\")\r\n",
    "        print(line)\r\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:22:55.294391Z",
     "iopub.status.busy": "2023-01-10T08:22:55.293660Z",
     "iopub.status.idle": "2023-01-10T08:22:55.324343Z",
     "shell.execute_reply": "2023-01-10T08:22:55.323626Z",
     "shell.execute_reply.started": "2023-01-10T08:22:55.294348Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-10 16:22:55,303] [    INFO] - Found /home/aistudio/.paddlenlp/models/bert-base-chinese/bert-base-chinese-vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(\"bert-base-chinese\")\r\n",
    "\r\n",
    "#数据预处理\r\n",
    "def convert_example(example,tokenizer,label_list,max_seq_length=256,is_test=False):\r\n",
    "    if is_test:\r\n",
    "        text = example\r\n",
    "    else:\r\n",
    "        text, label = example\r\n",
    "    #tokenizer.encode方法能够完成切分token，映射token ID以及拼接特殊token\r\n",
    "    encoded_inputs = tokenizer.encode(text=text, max_seq_len=max_seq_length)\r\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\r\n",
    "    #注意，在早前的PaddleNLP版本中，token_type_ids叫做segment_ids\r\n",
    "    segment_ids = encoded_inputs[\"segment_ids\"]\r\n",
    "\r\n",
    "    if not is_test:\r\n",
    "        label_map = {}\r\n",
    "        for (i, l) in enumerate(label_list):\r\n",
    "            label_map[l] = i\r\n",
    "\r\n",
    "        label = label_map[label]\r\n",
    "        label = np.array([label], dtype=\"int64\")\r\n",
    "        return input_ids, segment_ids, label\r\n",
    "    else:\r\n",
    "        return input_ids, segment_ids\r\n",
    "\r\n",
    "#数据迭代器构造方法\r\n",
    "def create_dataloader(dataset, trans_fn=None, mode='train', batch_size=1, use_gpu=False, pad_token_id=0, batchify_fn=None):\r\n",
    "    if trans_fn:\r\n",
    "        dataset = dataset.apply(trans_fn, lazy=True)\r\n",
    "\r\n",
    "    if mode == 'train' and use_gpu:\r\n",
    "        sampler = paddle.io.DistributedBatchSampler(dataset=dataset, batch_size=batch_size, shuffle=True)\r\n",
    "    else:\r\n",
    "        shuffle = True if mode == 'train' else False #如果不是训练集，则不打乱顺序\r\n",
    "        sampler = paddle.io.BatchSampler(dataset=dataset, batch_size=batch_size, shuffle=shuffle) #生成一个取样器\r\n",
    "    dataloader = paddle.io.DataLoader(dataset, batch_sampler=sampler, return_list=True, collate_fn=batchify_fn)\r\n",
    "    return dataloader\r\n",
    "\r\n",
    "#使用partial()来固定convert_example函数的tokenizer, label_list, max_seq_length, is_test等参数值\r\n",
    "trans_fn = partial(convert_example, tokenizer=tokenizer, label_list=label_list, max_seq_length=128, is_test=False)\r\n",
    "batchify_fn = lambda samples, fn=Tuple(Pad(axis=0,pad_val=tokenizer.pad_token_id), Pad(axis=0, pad_val=tokenizer.pad_token_id), Stack(dtype=\"int64\")):[data for data in fn(samples)]\r\n",
    "#训练集迭代器\r\n",
    "train_loader = create_dataloader(train_ds, mode='train', batch_size=32, batchify_fn=batchify_fn, trans_fn=trans_fn)\r\n",
    "#验证集迭代器\r\n",
    "dev_loader = create_dataloader(dev_ds, mode='dev', batch_size=1, batchify_fn=batchify_fn, trans_fn=trans_fn)\r\n",
    "#测试集迭代器\r\n",
    "test_loader = create_dataloader(test_ds, mode='test', batch_size=32, batchify_fn=batchify_fn, trans_fn=trans_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:17:56.783462Z",
     "iopub.status.busy": "2023-01-10T08:17:56.782767Z",
     "iopub.status.idle": "2023-01-10T08:19:32.776938Z",
     "shell.execute_reply": "2023-01-10T08:19:32.775802Z",
     "shell.execute_reply.started": "2023-01-10T08:17:56.783414Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-01-10 16:17:56,785] [    INFO] - Downloading http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-base-chinese.pdparams and saved to /home/aistudio/.paddlenlp/models/bert-base-chinese\r\n",
      "[2023-01-10 16:17:56,790] [    INFO] - Downloading bert-base-chinese.pdparams from http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-base-chinese.pdparams\r\n",
      "100%|██████████| 696494/696494 [01:27<00:00, 8004.19it/s] \r\n",
      "W0110 16:19:23.928725   270 device_context.cc:362] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\r\n",
      "W0110 16:19:23.935091   270 device_context.cc:372] device: 0, cuDNN Version: 7.6.\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1303: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\r\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\r\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1303: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\r\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\r\n"
     ]
    }
   ],
   "source": [
    "model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:19:58.650111Z",
     "iopub.status.busy": "2023-01-10T08:19:58.649452Z",
     "iopub.status.idle": "2023-01-10T08:19:58.659683Z",
     "shell.execute_reply": "2023-01-10T08:19:58.658667Z",
     "shell.execute_reply.started": "2023-01-10T08:19:58.650067Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#设置训练超参数\r\n",
    "\r\n",
    "#学习率\r\n",
    "learning_rate = 1e-5 \r\n",
    "#训练轮次\r\n",
    "epochs = 3\r\n",
    "#学习率预热比率\r\n",
    "warmup_proption = 0.1\r\n",
    "#权重衰减系数\r\n",
    "weight_decay = 0.01\r\n",
    "\r\n",
    "num_training_steps = len(train_loader) * epochs\r\n",
    "num_warmup_steps = int(warmup_proption * num_training_steps)\r\n",
    "\r\n",
    "def get_lr_factor(current_step):\r\n",
    "    if current_step < num_warmup_steps:\r\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\r\n",
    "    else:\r\n",
    "        return max(0.0,\r\n",
    "                    float(num_training_steps - current_step) /\r\n",
    "                    float(max(1, num_training_steps - num_warmup_steps)))\r\n",
    "#学习率调度器\r\n",
    "lr_scheduler = paddle.optimizer.lr.LambdaDecay(learning_rate, lr_lambda=lambda current_step: get_lr_factor(current_step))\r\n",
    "\r\n",
    "#优化器\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=lr_scheduler,\r\n",
    "    parameters=model.parameters(),\r\n",
    "    weight_decay=weight_decay,\r\n",
    "    apply_decay_param_fun=lambda x: x in [\r\n",
    "        p.name for n, p in model.named_parameters()\r\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\r\n",
    "    ])\r\n",
    "\r\n",
    "#损失函数\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\r\n",
    "#评估函数\r\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:20:01.490584Z",
     "iopub.status.busy": "2023-01-10T08:20:01.489515Z",
     "iopub.status.idle": "2023-01-10T08:20:01.496270Z",
     "shell.execute_reply": "2023-01-10T08:20:01.495532Z",
     "shell.execute_reply.started": "2023-01-10T08:20:01.490543Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, criterion, metric, data_loader):\r\n",
    "    model.eval()\r\n",
    "    metric.reset()\r\n",
    "    losses = []\r\n",
    "    for batch in data_loader:\r\n",
    "        input_ids, segment_ids, labels = batch\r\n",
    "        logits = model(input_ids, segment_ids)\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "        losses.append(loss.numpy())\r\n",
    "        correct = metric.compute(logits, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        accu = metric.accumulate()\r\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\r\n",
    "    model.train()\r\n",
    "    metric.reset()\r\n",
    "    return np.mean(losses), accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:20:03.919170Z",
     "iopub.status.busy": "2023-01-10T08:20:03.917993Z",
     "iopub.status.idle": "2023-01-10T08:21:16.618596Z",
     "shell.execute_reply": "2023-01-10T08:21:16.617770Z",
     "shell.execute_reply.started": "2023-01-10T08:20:03.919114Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 50, epoch: 1, batch: 50, loss: 0.30604, acc: 0.68188\r\n",
      "eval loss: 0.29938, accu: 0.88791\r\n",
      "global step 100, epoch: 2, batch: 15, loss: 0.33418, acc: 0.89167\r\n",
      "global step 150, epoch: 2, batch: 65, loss: 0.18857, acc: 0.91154\r\n",
      "eval loss: 0.25920, accu: 0.90265\r\n",
      "global step 200, epoch: 3, batch: 30, loss: 0.13239, acc: 0.94792\r\n",
      "global step 250, epoch: 3, batch: 80, loss: 0.06305, acc: 0.95273\r\n",
      "eval loss: 0.25578, accu: 0.91150\r\n"
     ]
    }
   ],
   "source": [
    "global_step = 0\r\n",
    "with LogWriter(logdir=\"./log\") as writer:\r\n",
    "    for epoch in range(1, epochs + 1):    \r\n",
    "        for step, batch in enumerate(train_loader, start=1): #从训练数据迭代器中取数据\r\n",
    "            input_ids, segment_ids, labels = batch\r\n",
    "            #print(segment_ids)\r\n",
    "            logits = model(input_ids, segment_ids)\r\n",
    "            loss = criterion(logits, labels) #计算损失\r\n",
    "            probs = F.softmax(logits, axis=1)\r\n",
    "            correct = metric.compute(probs, labels)\r\n",
    "            metric.update(correct)\r\n",
    "            acc = metric.accumulate()\r\n",
    "            # print(acc)\r\n",
    "\r\n",
    "            global_step += 1\r\n",
    "            if global_step % 50 == 0 :\r\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\r\n",
    "                #记录训练过程\r\n",
    "                writer.add_scalar(tag=\"train/loss\", step=global_step, value=loss)\r\n",
    "                writer.add_scalar(tag=\"train/acc\", step=global_step, value=acc)\r\n",
    "            loss.backward()\r\n",
    "            optimizer.step()\r\n",
    "            lr_scheduler.step()\r\n",
    "            optimizer.clear_gradients()\r\n",
    "        eval_loss, eval_acc = evaluate(model, criterion, metric, dev_loader)\r\n",
    "        #记录评估过程\r\n",
    "        writer.add_scalar(tag=\"eval/loss\", step=epoch, value=eval_loss)\r\n",
    "        writer.add_scalar(tag=\"eval/acc\", step=epoch, value=eval_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:21:52.486847Z",
     "iopub.status.busy": "2023-01-10T08:21:52.486226Z",
     "iopub.status.idle": "2023-01-10T08:21:52.493055Z",
     "shell.execute_reply": "2023-01-10T08:21:52.492330Z",
     "shell.execute_reply.started": "2023-01-10T08:21:52.486804Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def predict(model, data, tokenizer, label_map, batch_size=1):\r\n",
    "    examples = []\r\n",
    "    results = []\r\n",
    "    lable = []\r\n",
    "    model.eval()\r\n",
    "    for batch in data:\r\n",
    "        input_ids, segment_ids, labels = batch\r\n",
    "        input_ids = paddle.to_tensor(input_ids)\r\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\r\n",
    "        logits = model(input_ids, segment_ids)\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\r\n",
    "        idx = idx.tolist()\r\n",
    "        # print(labels.numpy().tolist())\r\n",
    "        # labels = [label_map[i] for i in idx]\r\n",
    "        results.extend(idx)\r\n",
    "        lable.extend(labels.numpy().tolist()[0])\r\n",
    "    return results,lable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-10T08:23:01.831644Z",
     "iopub.status.busy": "2023-01-10T08:23:01.830856Z",
     "iopub.status.idle": "2023-01-10T08:23:23.334726Z",
     "shell.execute_reply": "2023-01-10T08:23:23.333913Z",
     "shell.execute_reply.started": "2023-01-10T08:23:01.831512Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\r\n",
      "\r\n",
      "           0     0.9048    0.9048    0.9048       315\r\n",
      "           1     0.9174    0.9174    0.9174       363\r\n",
      "\r\n",
      "    accuracy                         0.9115       678\r\n",
      "   macro avg     0.9111    0.9111    0.9111       678\r\n",
      "weighted avg     0.9115    0.9115    0.9115       678\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: '谣言', 1: '不是谣言'}\r\n",
    "predictions,r = predict(model, dev_loader, tokenizer, label_map, batch_size=1)\r\n",
    "\r\n",
    "from sklearn.metrics import classification_report\r\n",
    "target_names = [\"0\",\"1\"]\r\n",
    "CR=classification_report(r, predictions, target_names=target_names,digits=4)\r\n",
    "print(CR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
